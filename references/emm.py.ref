# Chunk 1/3 - emm.py - Enhanced Memory Manager with Narrative Time Support
#!/usr/bin/env python3

import json
import os
import shutil
import time
from datetime import datetime
from enum import Enum
from typing import Dict, List, Optional, Any, Tuple
from uuid import uuid4
import threading
import asyncio
import httpx

# Default memory file configuration
DEFAULT_MEMORY_FILE = "memory.json"

class MessageType(Enum):
    """Message type enumeration for conversation tracking"""
    USER = "user"
    ASSISTANT = "assistant"
    SYSTEM = "system"
    MOMENTUM_STATE = "momentum_state"  # Special type for SME state storage

class Message:
    """Individual conversation message with metadata"""
    
    def __init__(self, content: str, message_type: MessageType, timestamp: Optional[str] = None):
        self.content = content
        self.message_type = message_type
        self.timestamp = timestamp or datetime.now().isoformat()  # Real time for file metadata
        self.token_estimate = self._estimate_tokens(content)
        self.id = str(uuid4())
        self.content_category = "standard"  # Default category for semantic analysis
        self.condensed = False
        
        # Narrative time tracking (separate from real timestamp)
        self.narrative_sequence = 0  # Set by EMM when added
        self.narrative_duration = 10.0  # Default duration in seconds
    
    def _estimate_tokens(self, text: str) -> int:
        """Conservative token estimation for memory planning"""
        if not text:
            return 0
        # Rough estimation: 1 token per 4 characters
        return max(1, len(text) // 4)
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert message to dictionary for storage"""
        return {
            "id": self.id,
            "content": self.content,
            "type": self.message_type.value,
            "timestamp": self.timestamp,  # Real time for file operations
            "tokens": self.token_estimate,
            "content_category": self.content_category,
            "condensed": self.condensed,
            "narrative_sequence": self.narrative_sequence,
            "narrative_duration": self.narrative_duration
        }
    
    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> 'Message':
        """Create message from dictionary"""
        msg = cls(
            content=data["content"],
            message_type=MessageType(data["type"]),
            timestamp=data["timestamp"]
        )
        msg.id = data.get("id", str(uuid4()))
        msg.content_category = data.get("content_category", "standard")
        msg.condensed = data.get("condensed", False)
        msg.narrative_sequence = data.get("narrative_sequence", 0)
        msg.narrative_duration = data.get("narrative_duration", 10.0)
        return msg

# Semantic category preservation ratios (unchanged)
CONDENSATION_STRATEGIES = {
    "story_critical": {
        "threshold": 100,
        "preservation_ratio": 0.8,
        "instruction": (
            "Preserve all major plot developments, character deaths, world-changing events, "
            "key player decisions, and their consequences. Use decisive language highlighting "
            "the significance of events. Compress dialogue while maintaining essential meaning."
        )
    },
    "character_focused": {
        "threshold": 80,
        "preservation_ratio": 0.7,
        "instruction": (
            "Preserve relationship changes, trust/betrayal moments, character motivations, "
            "personality reveals, Aurora's development, and NPC traits. Emphasize emotional "
            "weight and relationship dynamics. Condense descriptions while keeping character essence."
        )
    },
    "relationship_dynamics": {
        "threshold": 80,
        "preservation_ratio": 0.8,
        "instruction": (
            "Preserve evolving relationships between characters, trust building, conflicts, "
            "alliances, and interpersonal dynamics. Maintain emotional context and progression."
        )
    },
    "emotional_significance": {
        "threshold": 70,
        "preservation_ratio": 0.75,
        "instruction": (
            "Preserve dramatic moments, emotional peaks, character growth, conflict resolution, "
            "and significant emotional revelations. Maintain the emotional weight of scenes."
        )
    },
    "world_building": {
        "threshold": 60,
        "preservation_ratio": 0.6,
        "instruction": (
            "Preserve new locations, lore revelations, cultural information, political changes, "
            "economic systems, magical discoveries, and historical context. Provide rich "
            "foundational details. Compress atmospheric descriptions while keeping key world facts."
        )
    },
    "standard": {
        "threshold": 40,
        "preservation_ratio": 0.4,
        "instruction": (
            "Preserve player actions and immediate consequences for continuity. Compress "
            "everything else aggressively while maintaining basic story flow."
        )
    }
}

class EnhancedMemoryManager:
    """Memory management with LLM-powered semantic condensation and narrative time tracking"""
    
    def __init__(self, max_memory_tokens: int = 16000, debug_logger=None, 
                 auto_save_enabled: bool = True, memory_file: str = DEFAULT_MEMORY_FILE):
        self.max_memory_tokens = max_memory_tokens
        self.debug_logger = debug_logger
        self.auto_save_enabled = auto_save_enabled
        self.memory_file = memory_file
        self.messages: List[Message] = []
        self.condensation_count = 0
        self.lock = threading.Lock()
        
        # Narrative time tracking
        self.current_narrative_sequence = 0
        self.total_narrative_time = 0.0
        
        # MCP client configuration
        self.mcp_config = self._load_mcp_config()
        
        # Auto-load existing memory on initialization
        if self.auto_save_enabled:
            self._auto_load()
    
    def _load_mcp_config(self) -> Dict[str, Any]:
        """Load MCP configuration for LLM calls"""
        return {
            "server_url": "http://127.0.0.1:3456/chat",
            "model": "qwen2.5:14b-instruct-q4_k_m",
            "timeout": 300
        }
    
    async def _call_llm(self, messages: List[Dict[str, str]]) -> Optional[str]:
        """Make LLM request for semantic analysis using working MCP format"""
        try:
            async with httpx.AsyncClient(timeout=self.mcp_config.get("timeout", 30)) as client:
                # Use same payload format as working mcp.py
                payload = {
                    "model": self.mcp_config["model"],
                    "messages": messages,
                    "stream": False
                }

                response = await client.post(self.mcp_config["server_url"], json=payload)
                response.raise_for_status()

                if response.status_code == 200:
                    result = response.json()
                    return result.get("message", {}).get("content", "")

        except Exception as e:
            if self.debug_logger:
                self.debug_logger.error(f"EMM LLM call failed: {e}")
            return None
    
    def _auto_load(self) -> None:
        """Auto-load memory from file if it exists"""
        try:
            if os.path.exists(self.memory_file):
                success = self.load_conversation(self.memory_file)
                if success and self.debug_logger:
                    self.debug_logger.debug(f"Auto-loaded {len(self.messages)} messages from {self.memory_file}")
                elif not success and self.debug_logger:
                    self.debug_logger.error(f"Failed to auto-load from {self.memory_file}")
            elif self.debug_logger:
                self.debug_logger.debug(f"No existing memory file found at {self.memory_file}")
        except Exception as e:
            if self.debug_logger:
                self.debug_logger.error(f"Auto-load error: {e}")
    
    def _auto_save(self) -> None:
        """Auto-save memory to file (real time operation)"""
        if not self.auto_save_enabled:
            return
            
        try:
            # Create backup of existing file
            if os.path.exists(self.memory_file):
                backup_file = f"{self.memory_file}.bak"
                shutil.copy2(self.memory_file, backup_file)
            
            # Save current state
            success = self.save_conversation(self.memory_file)
            if not success and self.debug_logger:
                self.debug_logger.error(f"Auto-save failed to {self.memory_file}")
                
        except Exception as e:
            if self.debug_logger:
                self.debug_logger.error(f"Auto-save error: {e}")
    
    def add_message(self, content: str, message_type: MessageType, narrative_duration: float = None) -> None:
        """Add new message with narrative time tracking and manage memory"""
        with self.lock:
            message = Message(content, message_type)
            
            # Set narrative time information
            self.current_narrative_sequence += 1
            message.narrative_sequence = self.current_narrative_sequence
            
            # Use provided duration or calculate default
            if narrative_duration is not None:
                message.narrative_duration = narrative_duration
            else:
                # Default duration based on message type
                if message_type == MessageType.USER:
                    message.narrative_duration = 10.0  # Default user action duration
                elif message_type == MessageType.ASSISTANT:
                    message.narrative_duration = 5.0   # GM responses don't consume narrative time
                else:
                    message.narrative_duration = 0.0   # System messages don't consume time
            
            # Update total narrative time
            self.total_narrative_time += message.narrative_duration
            
            self.messages.append(message)

            if self.debug_logger:
                self.debug_logger.debug(f"Added {message_type.value} message: {len(content)} chars, "
                                      f"{message.token_estimate} tokens, {message.narrative_duration:.1f}s narrative time")

        # Move ALL auto-save operations to background thread to avoid blocking main thread
        if self.auto_save_enabled:
            auto_save_thread = threading.Thread(
                target=self._background_auto_save,
                daemon=True,
                name="EMM-AutoSave"
            )
            auto_save_thread.start()

    def _background_auto_save(self) -> None:
        """Handle auto-save and condensation in background thread"""
        try:
            # Auto-save first (file operations - uses real time)
            self._auto_save()

            # Check if condensation needed
            with self.lock:
                current_tokens = sum(msg.token_estimate for msg in self.messages)

            if current_tokens > self.max_memory_tokens:
                if self.debug_logger:
                    self.debug_logger.debug(f"Starting background condensation: {current_tokens} > {self.max_memory_tokens}")
                self._perform_semantic_condensation()

        except Exception as e:
            if self.debug_logger:
                self.debug_logger.error(f"Background auto-save failed: {e}")
    
    def get_narrative_time_stats(self) -> Dict[str, Any]:
        """Get narrative time statistics"""
        with self.lock:
            conversation_messages = [msg for msg in self.messages 
                                   if msg.message_type in [MessageType.USER, MessageType.ASSISTANT]]
            
            if not conversation_messages:
                return {
                    "total_narrative_time": 0.0,
                    "narrative_time_formatted": "0s",
                    "conversation_count": 0,
                    "average_duration": 0.0
                }
            
            total_time = sum(msg.narrative_duration for msg in conversation_messages)
            avg_duration = total_time / len(conversation_messages) if conversation_messages else 0.0
            
            # Format time display
            if total_time < 60:
                time_formatted = f"{total_time:.0f}s"
            elif total_time < 3600:
                time_formatted = f"{total_time/60:.1f}m"
            else:
                time_formatted = f"{total_time/3600:.1f}h"
            
            return {
                "total_narrative_time": total_time,
                "narrative_time_formatted": time_formatted,
                "conversation_count": len(conversation_messages),
                "average_duration": avg_duration
            }

# Chunk 2/3 - emm.py - Semantic Analysis with Narrative Time Context

    def _perform_semantic_condensation(self) -> None:
        """Execute multi-pass LLM-powered semantic condensation with auto-save"""
        if len(self.messages) < 10:  # Need minimum messages for context
            return
            
        # Run async condensation in thread
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        try:
            loop.run_until_complete(self._async_multi_pass_condensation())
        finally:
            loop.close()
    
    async def _async_multi_pass_condensation(self) -> None:
        """Multi-pass async condensation with increasing aggressiveness and auto-save"""
        max_passes = 3
        
        for pass_num in range(max_passes):
            current_tokens = sum(msg.token_estimate for msg in self.messages)
            
            if current_tokens <= self.max_memory_tokens:
                if self.debug_logger:
                    self.debug_logger.debug(f"Condensation complete after {pass_num} passes: {current_tokens} tokens")
                break
                
            if self.debug_logger:
                self.debug_logger.debug(f"Condensation pass {pass_num + 1}/{max_passes}, tokens: {current_tokens}")
            
            # Ensure all messages have semantic categories
            await self._categorize_uncategorized_messages()
            
            # Collect preservation candidates with increasing aggressiveness
            preserve_messages, condense_candidates = await self._collect_preservation_candidates(pass_num)
            
            if not condense_candidates:
                if self.debug_logger:
                    self.debug_logger.debug(f"No condensation candidates found at aggressiveness level {pass_num}")
                continue
            
            # Create condensed summary for candidates by category
            condensed_message = await self._create_category_aware_summary(condense_candidates)
            
            if condensed_message:
                # Replace candidates with condensed summary
                with self.lock:
                    # Preserve narrative time continuity in condensed message
                    if condense_candidates:
                        first_msg = condense_candidates[0]
                        last_msg = condense_candidates[-1]
                        condensed_message.narrative_sequence = first_msg.narrative_sequence
                        # Sum up narrative durations of condensed messages
                        total_duration = sum(msg.narrative_duration for msg in condense_candidates)
                        condensed_message.narrative_duration = total_duration
                    
                    self.messages = [condensed_message] + preserve_messages
                    self.condensation_count += 1
                
                # Auto-save after successful condensation
                self._auto_save()
                
                new_tokens = sum(msg.token_estimate for msg in self.messages)
                if self.debug_logger:
                    self.debug_logger.debug(
                        f"Pass {pass_num + 1} complete: condensed {len(condense_candidates)} messages, "
                        f"tokens: {current_tokens} → {new_tokens}"
                    )
            else:
                if self.debug_logger:
                    self.debug_logger.debug(f"Condensation failed at pass {pass_num + 1}")
                break
        
        # Final status
        final_tokens = sum(msg.token_estimate for msg in self.messages)
        if final_tokens > self.max_memory_tokens:
            if self.debug_logger:
                self.debug_logger.debug(f"Condensation incomplete: {final_tokens} tokens still exceed limit")

    async def _categorize_uncategorized_messages(self) -> None:
        """Ensure all messages have semantic categories"""
        uncategorized = [
            (i, msg) for i, msg in enumerate(self.messages)
            if msg.message_type in [MessageType.USER, MessageType.ASSISTANT] 
            and msg.content_category == "standard"
            and not msg.condensed
        ]
        
        if self.debug_logger and uncategorized:
            self.debug_logger.debug(f"Categorizing {len(uncategorized)} uncategorized messages")
        
        # Process in batches to avoid overwhelming LLM
        batch_size = 10
        for batch_start in range(0, len(uncategorized), batch_size):
            batch = uncategorized[batch_start:batch_start + batch_size]
            
            for msg_idx, message in batch:
                analysis = await self._analyze_message_semantics(msg_idx)
                if analysis:
                    categories = analysis.get("categories", ["standard"])
                    # Use highest-priority category
                    message.content_category = self._get_highest_priority_category(categories)

    async def _analyze_message_semantics(self, target_idx: int) -> Optional[Dict[str, Any]]:
        """Analyze message semantics with context window - 3 retry attempts"""
        
        # Create context window (5 before + target + 5 after)
        start_idx = max(0, target_idx - 5)
        end_idx = min(len(self.messages), target_idx + 6)
        context_messages = self.messages[start_idx:end_idx]
        
        target_message = self.messages[target_idx]
        
        # Attempt 1: Full analysis with narrative time context
        prompt1 = self._create_full_analysis_prompt(context_messages, target_idx - start_idx)
        result = await self._call_llm([{"role": "system", "content": prompt1}])
        
        if result:
            parsed = self._parse_semantic_response_robust(result, attempt=1)
            if parsed:
                return parsed
        
        # Attempt 2: Simplified analysis
        prompt2 = self._create_simple_analysis_prompt(target_message.content)
        result = await self._call_llm([{"role": "system", "content": prompt2}])
        
        if result:
            parsed = self._parse_semantic_response_robust(result, attempt=2)
            if parsed:
                return parsed
        
        # Attempt 3: Binary preserve/condense decision
        prompt3 = self._create_binary_prompt(target_message.content)
        result = await self._call_llm([{"role": "system", "content": prompt3}])
        
        if result:
            parsed = self._parse_semantic_response_robust(result, attempt=3)
            if parsed:
                return parsed
        
        # All attempts failed - return default
        return {
            "importance_score": 0.4,
            "categories": ["standard"],
            "fragments": None
        }

    def _create_full_analysis_prompt(self, context_messages: List[Message], target_idx: int) -> str:
        """Create detailed semantic analysis prompt with narrative time context"""
        context_text = "\n".join([
            f"[{i}] {msg.message_type.value} ({msg.narrative_duration:.1f}s): {msg.content}"
            for i, msg in enumerate(context_messages)
        ])
        
        return f"""Analyze message [{target_idx}] in context for semantic importance and categorization.
This is from an RPG conversation where narrative time progression matters.

Context (with narrative durations):
{context_text}

Categories:
- story_critical: Major plot developments, character deaths, world-changing events
- character_focused: Relationship changes, character development, personality reveals
- relationship_dynamics: Evolving relationships between characters
- emotional_significance: Dramatic moments, trust/betrayal, conflict resolution
- world_building: New locations, lore, cultural info, political changes
- standard: General interactions, travel, routine activities

Consider narrative time when assessing importance - longer durations often indicate more significant actions.

Return JSON format:
{{
  "importance_score": 0.0-1.0,
  "categories": ["category1", "category2"],
  "narrative_significance": "brief explanation of time/importance relationship"
}}"""

    def _create_simple_analysis_prompt(self, content: str) -> str:
        """Create simplified analysis prompt"""
        return f"""Categorize this RPG message and rate its story importance:

Message: {content}

Categories: story_critical, character_focused, relationship_dynamics, emotional_significance, world_building, standard

Return JSON:
{{
  "importance_score": 0.0-1.0,
  "categories": ["primary_category"]
}}"""

    def _create_binary_prompt(self, content: str) -> str:
        """Create binary preserve/condense prompt"""
        return f"""Should this RPG message be preserved or condensed for story continuity?

Message: {content}

Return JSON:
{{
  "preserve": true/false
}}"""

    def _parse_semantic_response_robust(self, response: str, attempt: int) -> Optional[Dict[str, Any]]:
        """Parse LLM semantic analysis response with 5-strategy defensive handling"""
        
        # Strategy 1: Direct JSON parsing
        try:
            data = json.loads(response.strip())
            if self._validate_semantic_data(data, attempt):
                return self._inject_missing_fields(data, attempt)
        except json.JSONDecodeError:
            pass
        
        # Strategy 2: Substring extraction
        try:
            start = response.find('{')
            end = response.rfind('}') + 1
            
            if start >= 0 and end > start:
                json_str = response[start:end]
                data = json.loads(json_str)
                if self._validate_semantic_data(data, attempt):
                    return self._inject_missing_fields(data, attempt)
        except (json.JSONDecodeError, ValueError):
            pass
        
        # Strategy 3: Field validation and extraction
        try:
            # Look for specific patterns in response
            if attempt == 3:  # Binary response
                if "true" in response.lower() or "preserve" in response.lower():
                    return {"importance_score": 0.8, "categories": ["story_critical"], "fragments": None}
                else:
                    return {"importance_score": 0.2, "categories": ["standard"], "fragments": None}
            
            # Try to extract numeric importance
            import re
            importance_match = re.search(r'"?importance_score"?\s*:\s*([0-9.]+)', response)
            if importance_match:
                importance = float(importance_match.group(1))
                return {"importance_score": importance, "categories": ["standard"], "fragments": None}
        except:
            pass
        
        # Strategy 4: Default injection based on attempt type
        if attempt == 3:  # Binary response
            return {"importance_score": 0.4, "categories": ["standard"], "fragments": None}
        elif attempt == 2:  # Simple response
            return {"importance_score": 0.4, "categories": ["standard"], "fragments": None}
        
        # Strategy 5: Complete fallback
        return {"importance_score": 0.4, "categories": ["standard"], "fragments": None}

    def _validate_semantic_data(self, data: Dict[str, Any], attempt: int) -> bool:
        """Validate that semantic analysis data has required fields"""
        if not isinstance(data, dict):
            return False
        
        if attempt == 3:  # Binary response
            return "preserve" in data
        
        required_fields = ["importance_score", "categories"]
        return all(field in data for field in required_fields)

    def _inject_missing_fields(self, data: Dict[str, Any], attempt: int) -> Dict[str, Any]:
        """Inject missing fields with sensible defaults"""
        if attempt == 3:  # Binary response
            preserve = data.get("preserve", False)
            return {
                "importance_score": 0.8 if preserve else 0.2,
                "categories": ["story_critical"] if preserve else ["standard"],
                "fragments": None
            }
        
        # Ensure importance_score is valid
        importance = data.get("importance_score", 0.4)
        if not isinstance(importance, (int, float)) or importance < 0 or importance > 1:
            importance = 0.4
        data["importance_score"] = importance
        
        # Ensure categories is a list
        categories = data.get("categories", ["standard"])
        if not isinstance(categories, list):
            categories = ["standard"]
        data["categories"] = categories
        
        # Ensure fragments field exists
        if "fragments" not in data:
            data["fragments"] = None
        
        return data

    def _get_highest_priority_category(self, categories: List[str]) -> str:
        """Get highest priority category from list"""
        priority_order = [
            "story_critical", "character_focused", "relationship_dynamics", 
            "emotional_significance", "world_building", "standard"
        ]
        
        for category in priority_order:
            if category in categories:
                return category
        return "standard"

# Chunk 3/3 - emm.py - Memory Operations and State Management with Narrative Time

    async def _collect_preservation_candidates(self, aggressiveness_level: int) -> Tuple[List[Message], List[Message]]:
        """Collect messages for preservation vs condensation with increasing aggressiveness"""
        preserve_messages = []
        condense_candidates = []
        
        # Always preserve recent messages (last 5)
        recent_cutoff = max(0, len(self.messages) - 5)
        
        for i, message in enumerate(self.messages[:recent_cutoff]):
            # Skip already condensed messages
            if message.condensed:
                preserve_messages.append(message)
                continue
            
            # Skip non-conversation messages
            if message.message_type not in [MessageType.USER, MessageType.ASSISTANT]:
                preserve_messages.append(message)
                continue
            
            # Analyze message for preservation decision
            analysis = await self._analyze_message_semantics(i)
            should_preserve = self._should_preserve_with_aggressiveness(analysis, aggressiveness_level)
            
            if should_preserve:
                preserve_messages.append(message)
            else:
                condense_candidates.append(message)
        
        # Always preserve recent messages
        preserve_messages.extend(self.messages[recent_cutoff:])
        
        if self.debug_logger:
            self.debug_logger.debug(
                f"Aggressiveness {aggressiveness_level}: preserve {len(preserve_messages)}, "
                f"condense {len(condense_candidates)}"
            )
        
        return preserve_messages, condense_candidates

    def _should_preserve_with_aggressiveness(self, analysis: Dict[str, Any], aggressiveness: int) -> bool:
        """Determine preservation with increasing aggressiveness"""
        categories = analysis.get("categories", ["standard"])
        importance = analysis.get("importance_score", 0.4)
        
        # Get base preservation ratio for highest priority category
        highest_category = self._get_highest_priority_category(categories)
        base_ratio = CONDENSATION_STRATEGIES.get(highest_category, CONDENSATION_STRATEGIES["standard"])["preservation_ratio"]
        
        # Apply aggressiveness reduction
        # Pass 0: base ratio, Pass 1: -0.15, Pass 2: -0.3
        aggressiveness_reduction = aggressiveness * 0.15
        adjusted_ratio = max(0.1, base_ratio - aggressiveness_reduction)
        
        # Preserve if importance exceeds adjusted threshold
        preserve = importance >= adjusted_ratio
        
        return preserve

    async def _create_category_aware_summary(self, messages: List[Message]) -> Optional[Message]:
        """Create condensed summary organized by semantic categories"""
        if not messages:
            return None
        
        # Group messages by category
        category_groups = {}
        for msg in messages:
            category = msg.content_category
            if category not in category_groups:
                category_groups[category] = []
            category_groups[category].append(msg)
        
        # Create summaries for each category
        category_summaries = []
        
        for category, category_messages in category_groups.items():
            strategy = CONDENSATION_STRATEGIES.get(category, CONDENSATION_STRATEGIES["standard"])
            instruction = strategy["instruction"]
            
            content_to_condense = "\n".join([
                f"[{msg.message_type.value}] {msg.content}"
                for msg in category_messages
            ])
            
            prompt = f"""Condense the following {category} conversation content according to these guidelines:

{instruction}

Content to condense:
{content_to_condense}

Return a concise summary that preserves the essential elements for this category while minimizing length. Maintain narrative continuity and emotional context."""
            
            summary_content = await self._call_llm([{"role": "system", "content": prompt}])
            
            if summary_content:
                category_summaries.append(f"[{category.upper()}] {summary_content}")
        
        if category_summaries:
            final_summary = "\n\n".join(category_summaries)
            condensed_msg = Message(
                content=f"[CONDENSED - {len(messages)} messages] {final_summary}",
                message_type=MessageType.SYSTEM
            )
            condensed_msg.condensed = True
            condensed_msg.content_category = "condensed_summary"
            return condensed_msg
        
        return None

    def get_messages(self, limit: Optional[int] = None) -> List[Message]:
        """Retrieve messages with optional limit"""
        with self.lock:
            if limit:
                return self.messages[-limit:]
            return self.messages.copy()

    def get_conversation_for_mcp(self) -> List[Dict[str, str]]:
        """Format conversation for MCP requests, excluding momentum state"""
        with self.lock:
            return [
                {"role": msg.message_type.value, "content": msg.content}
                for msg in self.messages
                if msg.message_type != MessageType.MOMENTUM_STATE
            ]

    def get_memory_stats(self) -> Dict[str, Any]:
        """Return current memory statistics with narrative time"""
        with self.lock:
            total_tokens = sum(msg.token_estimate for msg in self.messages)
            narrative_stats = self.get_narrative_time_stats()
            
            return {
                "message_count": len(self.messages),
                "total_tokens": total_tokens,
                "max_tokens": self.max_memory_tokens,
                "utilization": total_tokens / self.max_memory_tokens,
                "condensations_performed": self.condensation_count,
                "narrative_time": narrative_stats.get("total_narrative_time", 0.0),
                "narrative_time_formatted": narrative_stats.get("narrative_time_formatted", "0s")
            }

    def save_conversation(self, filename: Optional[str] = None) -> bool:
        """Save conversation to file with robust error handling"""
        if not filename:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"chat_history_{timestamp}.json"
        
        try:
            with self.lock:
                narrative_stats = self.get_narrative_time_stats()
                conversation_data = {
                    "metadata": {
                        "saved_at": datetime.now().isoformat(),  # Real time for file metadata
                        "message_count": len(self.messages),
                        "total_tokens": sum(msg.token_estimate for msg in self.messages),
                        "condensations": self.condensation_count,
                        "auto_save_enabled": self.auto_save_enabled,
                        "narrative_time_total": narrative_stats.get("total_narrative_time", 0.0),
                        "narrative_time_formatted": narrative_stats.get("narrative_time_formatted", "0s"),
                        "current_narrative_sequence": self.current_narrative_sequence
                    },
                    "messages": [msg.to_dict() for msg in self.messages]
                }
            
            # Create backup if file already exists
            if os.path.exists(filename):
                backup_filename = f"{filename}.bak"
                shutil.copy2(filename, backup_filename)
            
            # Write to temporary file first, then move to prevent corruption
            temp_filename = f"{filename}.tmp"
            with open(temp_filename, "w", encoding="utf-8") as f:
                json.dump(conversation_data, f, indent=2, ensure_ascii=False)
            
            # Atomic move
            shutil.move(temp_filename, filename)
            
            if self.debug_logger:
                self.debug_logger.debug(f"Conversation saved to {filename}")
            
            return True
            
        except Exception as e:
            if self.debug_logger:
                self.debug_logger.error(f"Failed to save conversation: {e}")
            
            # Clean up temporary file if it exists
            temp_filename = f"{filename}.tmp"
            if os.path.exists(temp_filename):
                try:
                    os.remove(temp_filename)
                except:
                    pass
            
            return False

    def load_conversation(self, filename: str) -> bool:
        """Load conversation from file with corruption recovery and narrative time restoration"""
        try:
            if not os.path.exists(filename):
                if self.debug_logger:
                    self.debug_logger.debug(f"File does not exist: {filename}")
                return False
            
            # Try loading main file
            try:
                with open(filename, "r", encoding="utf-8") as f:
                    data = json.load(f)
            except (json.JSONDecodeError, OSError) as e:
                if self.debug_logger:
                    self.debug_logger.error(f"Main file corrupted, trying backup: {e}")
                
                # Try backup file
                backup_filename = f"{filename}.bak"
                if os.path.exists(backup_filename):
                    with open(backup_filename, "r", encoding="utf-8") as f:
                        data = json.load(f)
                    if self.debug_logger:
                        self.debug_logger.debug("Recovered from backup file")
                else:
                    raise e
            
            with self.lock:
                # Load messages
                self.messages = [
                    Message.from_dict(msg_data) 
                    for msg_data in data.get("messages", [])
                ]
                
                # Load metadata including narrative time tracking
                metadata = data.get("metadata", {})
                self.condensation_count = metadata.get("condensations", 0)
                self.current_narrative_sequence = metadata.get("current_narrative_sequence", len(self.messages))
                
                # Recalculate total narrative time from messages
                self.total_narrative_time = sum(
                    msg.narrative_duration for msg in self.messages 
                    if msg.message_type in [MessageType.USER, MessageType.ASSISTANT]
                )
            
            if self.debug_logger:
                narrative_time = metadata.get("narrative_time_formatted", "unknown")
                self.debug_logger.debug(f"Conversation loaded from {filename}: {len(self.messages)} messages, "
                                      f"narrative time: {narrative_time}")
            
            return True
            
        except Exception as e:
            if self.debug_logger:
                self.debug_logger.error(f"Failed to load conversation from {filename}: {e}")
            return False

    def clear_memory_file(self) -> bool:
        """Clear memory file and reset in-memory state"""
        try:
            with self.lock:
                # Clear in-memory state including narrative time
                self.messages.clear()
                self.condensation_count = 0
                self.current_narrative_sequence = 0
                self.total_narrative_time = 0.0
                
                # Remove memory file if it exists
                if os.path.exists(self.memory_file):
                    os.remove(self.memory_file)
                
                # Remove backup file if it exists
                backup_file = f"{self.memory_file}.bak"
                if os.path.exists(backup_file):
                    os.remove(backup_file)
            
            if self.debug_logger:
                self.debug_logger.debug(f"Memory file {self.memory_file} cleared with narrative time reset")
            
            return True
            
        except Exception as e:
            if self.debug_logger:
                self.debug_logger.error(f"Failed to clear memory file: {e}")
            return False

    def get_memory_file_info(self) -> Dict[str, Any]:
        """Get memory file information for status reporting"""
        try:
            if not os.path.exists(self.memory_file):
                return {
                    "file_exists": False,
                    "file_path": self.memory_file,
                    "auto_save_enabled": self.auto_save_enabled
                }
            
            stat = os.stat(self.memory_file)
            narrative_stats = self.get_narrative_time_stats()
            
            with self.lock:
                return {
                    "file_exists": True,
                    "file_path": self.memory_file,
                    "file_size": stat.st_size,
                    "last_modified": datetime.fromtimestamp(stat.st_mtime).isoformat(),
                    "message_count": len(self.messages),
                    "auto_save_enabled": self.auto_save_enabled,
                    "backup_exists": os.path.exists(f"{self.memory_file}.bak"),
                    "narrative_time": narrative_stats.get("narrative_time_formatted", "0s"),
                    "narrative_sequence": self.current_narrative_sequence
                }
                
        except Exception as e:
            if self.debug_logger:
                self.debug_logger.error(f"Failed to get memory file info: {e}")
            return {
                "file_exists": False,
                "error": str(e),
                "auto_save_enabled": self.auto_save_enabled
            }

    def analyze_conversation_patterns(self) -> Dict[str, Any]:
        """Generate conversation statistics for debugging with narrative time analysis"""
        with self.lock:
            if not self.messages:
                return {"status": "no_messages"}
            
            message_types = {}
            category_counts = {}
            narrative_durations = []
            
            for msg in self.messages:
                msg_type = msg.message_type.value
                message_types[msg_type] = message_types.get(msg_type, 0) + 1
                
                category = getattr(msg, 'content_category', 'unknown')
                category_counts[category] = category_counts.get(category, 0) + 1
                
                # Collect narrative durations for analysis
                if msg.message_type in [MessageType.USER, MessageType.ASSISTANT]:
                    narrative_durations.append(msg.narrative_duration)
            
            total_tokens = sum(msg.token_estimate for msg in self.messages)
            avg_tokens = total_tokens / len(self.messages) if self.messages else 0
            
            condensed_count = sum(1 for msg in self.messages if getattr(msg, 'condensed', False))
            narrative_stats = self.get_narrative_time_stats()
            
            return {
                "total_messages": len(self.messages),
                "message_types": message_types,
                "semantic_categories": category_counts,
                "condensed_messages": condensed_count,
                "total_tokens": total_tokens,
                "average_tokens_per_message": round(avg_tokens, 2),
                "memory_utilization": round(total_tokens / self.max_memory_tokens, 3),
                "condensations_performed": self.condensation_count,
                "oldest_message": self.messages[0].timestamp if self.messages else None,
                "newest_message": self.messages[-1].timestamp if self.messages else None,
                "auto_save_enabled": self.auto_save_enabled,
                "memory_file": self.memory_file,
                "narrative_time_stats": narrative_stats,
                "avg_narrative_duration": sum(narrative_durations) / len(narrative_durations) if narrative_durations else 0.0
            }

    # SME Integration Methods with narrative time
    def get_momentum_state(self) -> Optional[Dict[str, Any]]:
        """Retrieve current momentum state from memory for SME"""
        with self.lock:
            for message in reversed(self.messages):
                if message.message_type == MessageType.MOMENTUM_STATE:
                    try:
                        if isinstance(message.content, str):
                            return json.loads(message.content)
                        else:
                            return message.content
                    except json.JSONDecodeError:
                        continue
            return None

    def update_momentum_state(self, state_data: Dict[str, Any]) -> None:
        """Update or create momentum state in memory for SME"""
        with self.lock:
            # Remove existing momentum state
            self.messages = [msg for msg in self.messages if msg.message_type != MessageType.MOMENTUM_STATE]
            
            # Add new momentum state (doesn't consume narrative time)
            momentum_msg = Message(
                content=json.dumps(state_data),
                message_type=MessageType.MOMENTUM_STATE
            )
            momentum_msg.narrative_duration = 0.0  # State updates don't consume narrative time
            self.messages.append(momentum_msg)
        
        # Trigger auto-save for momentum state
        if self.auto_save_enabled:
            self._auto_save()

# Module test functionality
if __name__ == "__main__":
    print("DevName RPG Client - Enhanced Memory Manager with Narrative Time Tracking")
    print("Testing memory management functionality...")
    
    # Test with auto-save enabled
    emm = EnhancedMemoryManager(auto_save_enabled=True, memory_file="test_memory.json")
    
    # Test basic functionality with narrative time
    emm.add_message("Hello there!", MessageType.USER, narrative_duration=8.0)
    emm.add_message("Greetings, traveler!", MessageType.ASSISTANT, narrative_duration=5.0)
    
    stats = emm.get_memory_stats()
    print(f"Memory stats: {stats}")
    
    narrative_stats = emm.get_narrative_time_stats()
    print(f"Narrative time stats: {narrative_stats}")
    
    file_info = emm.get_memory_file_info()
    print(f"Memory file info: {file_info}")
    
    patterns = emm.analyze_conversation_patterns()
    print(f"Conversation patterns: {patterns}")
    
    # Test SME integration
    test_state = {
        "narrative_pressure": 0.3,
        "pressure_source": "antagonist",
        "manifestation_type": "tension",
        "escalation_count": 1,
        "base_pressure_floor": 0.0,
        "last_analysis_count": 5,
        "antagonist": {"name": "Test Villain", "motivation": "test purposes"}
    }
    
    emm.update_momentum_state(test_state)
    retrieved_state = emm.get_momentum_state()
    print(f"SME state integration: {retrieved_state}")
    
    # Test memory clearing
    emm.clear_memory_file()
    
    print("Memory manager test completed successfully with narrative time tracking.")
