# Chunk 1/5 - orch.py - Header, Imports, and Initial Class Setup (Debug Logger Fix)

#!/usr/bin/env python3
"""
DevName RPG Client - Orchestrator Module (orch.py)
Hub-and-spoke coordination for all service modules with standardized debug logging
FIXED: All debug logger calls use method pattern with null safety checks
"""

import threading
import time
import sys
import queue
from typing import Dict, Any, Optional, Callable, List, Tuple, Union
from pathlib import Path
from enum import Enum
from dataclasses import dataclass, field

# Ensure current directory is in Python path for local imports
current_dir = Path(__file__).parent.absolute()
if str(current_dir) not in sys.path:
    sys.path.insert(0, str(current_dir))

# Import service modules - orchestrator coordinates everything
try:
    import mcp
    from emm import EnhancedMemoryManager, MessageType
    from sme import StoryMomentumEngine  
    from sem import SemanticAnalysisEngine
    from ncui import NCursesUIController
except ImportError as e:
    print(f"Failed to import required service module: {e}")
    print("Ensure all remodularized module files are present in current directory")
    print("Required files: mcp.py, emm.py, sme.py, sem.py, ncui.py")
    raise

# =============================================================================
# ORCHESTRATOR STATE MANAGEMENT
# =============================================================================

class OrchestrationPhase(Enum):
    """Current operational phase of the orchestrator"""
    INITIALIZING = "initializing"
    ACTIVE = "active" 
    ANALYZING = "analyzing"
    SHUTTING_DOWN = "shutting_down"

@dataclass
class OrchestrationState:
    """Central state tracking for orchestrator operations"""
    phase: OrchestrationPhase = OrchestrationPhase.INITIALIZING
    message_count: int = 0
    last_analysis_count: int = 0
    analysis_in_progress: bool = False
    startup_complete: bool = False

    # Analysis tracking
    pending_analysis_requests: List[str] = field(default_factory=list)
    analysis_results: Dict[str, Any] = field(default_factory=dict)

    # Threading coordination
    background_threads: List[threading.Thread] = field(default_factory=list)
    shutdown_requested: bool = False

    # LLM Queue Management
    llm_queue: queue.PriorityQueue = field(default_factory=lambda: queue.PriorityQueue())
    llm_worker_thread: Optional[threading.Thread] = None
    llm_worker_shutdown: threading.Event = field(default_factory=threading.Event)
    llm_processing_lock: threading.Lock = field(default_factory=threading.Lock)
    current_llm_request: Optional['LLMRequest'] = None  # FIXED: Use string annotation

    # Request tracking
    next_request_id: int = 1
    completed_requests: Dict[str, Any] = field(default_factory=dict)
    failed_requests: Dict[str, Any] = field(default_factory=dict)

class LLMRequestType(Enum):
    """Types of LLM requests that can be queued"""
    USER_RESPONSE = "user_response"
    SEMANTIC_ANALYSIS = "semantic_analysis"
    CONDENSATION = "condensation"

@dataclass
class LLMRequest:
    """Request structure for LLM queue"""
    request_type: LLMRequestType
    user_input: str
    context_data: Dict[str, Any]
    callback: Optional[Callable] = None
    priority: int = 5  # Lower numbers = higher priority
    request_id: str = ""
    timestamp: float = 0.0

# =============================================================================
# MAIN ORCHESTRATOR CLASS
# =============================================================================

class Orchestrator:
    """
    Central coordination hub for all service modules.
    Implements hub-and-spoke pattern - no direct module-to-module communication.
    """
    
    def __init__(self, config: Dict[str, Any], loaded_prompts: Dict[str, str], debug_logger=None):
        """Initialize orchestrator with configuration and prompts"""
        self.config = config
        self.loaded_prompts = loaded_prompts
        self.debug_logger = debug_logger
        self.state = OrchestrationState()
        
        # Analysis configuration
        self.ANALYSIS_INTERVAL = 15  # Trigger analysis every 15 messages
        self.analysis_shutdown_event = threading.Event()
        self.analysis_thread = None
        
        # Service modules (spoke modules)
        self.memory_manager = None
        self.momentum_engine = None
        self.semantic_engine = None
        self.ui_controller = None
        self.mcp_client = None
        
        self._log_debug("Orchestrator created")

# Chunk 2/5 - orch.py - Module Initialization (Debug Logger Fix)

    def initialize_modules(self) -> bool:
        """
        Initialize all service modules in correct dependency order
        Hub-and-spoke pattern: orchestrator coordinates all modules
        """
        try:
            self._log_debug("Starting module initialization")
            
            # 1. Enhanced Memory Manager (storage only, no dependencies)
            self.memory_manager = EnhancedMemoryManager(debug_logger=self.debug_logger)
            self._log_debug("Memory manager initialized")
            
            # 2. Semantic Analysis Engine (analysis only, no dependencies)
            self.semantic_engine = SemanticAnalysisEngine(debug_logger=self.debug_logger)
            self._log_debug("Semantic engine initialized")
            
            # 3. Story Momentum Engine (state tracking with threading.Lock fix)
            self.momentum_engine = StoryMomentumEngine(debug_logger=self.debug_logger)
            self._log_debug("Momentum engine initialized")
            
            # 4. MCP client (exclusive orchestrator access)
            self.mcp_client = mcp.MCPClient(debug_logger=self.debug_logger)
            self._configure_mcp_client()
            self._log_debug("MCP client initialized")
            
            # 5. UI Controller (receives orchestrator callback)
            self.ui_controller = NCursesUIController(
                orchestrator_callback=self._handle_ui_callback,
                debug_logger=self.debug_logger
            )
            self._log_debug("UI controller initialized")
            
            # 6. Set orchestrator callbacks for all modules that need them
            self._setup_module_callbacks()
            
            # 7. Start background threads
            self._start_background_services()
            
            self.state.phase = OrchestrationPhase.ACTIVE
            self.state.startup_complete = True
            
            self._log_debug("All modules initialized successfully")
            return True
            
        except Exception as e:
            self._log_error(f"Module initialization failed: {e}")
            return False
    
    def _configure_mcp_client(self):
        """Configure MCP client with loaded prompts and settings - FIXED: Concatenate all prompts"""
        if not self.mcp_client:
            return

        try:
            # Build concatenated system prompt from all loaded prompts
            system_prompt_parts = []

            # Start with critrules (required)
            if self.loaded_prompts.get('critrules'):
                system_prompt_parts.append(self.loaded_prompts['critrules'])
                self._log_debug("Added critrules to system prompt")

            # Add companion prompt if available
            if self.loaded_prompts.get('companion'):
                system_prompt_parts.append(self.loaded_prompts['companion'])
                self._log_debug("Added companion to system prompt")

            # Add lowrules prompt if available
            if self.loaded_prompts.get('lowrules'):
                system_prompt_parts.append(self.loaded_prompts['lowrules'])
                self._log_debug("Added lowrules to system prompt")

            # Concatenate with two newlines between each prompt section
            if system_prompt_parts:
                concatenated_prompt = '/no_think\n\n' + '\n\n'.join(system_prompt_parts)
                self.mcp_client.system_prompt = concatenated_prompt
                self._log_debug(f"System prompt configured: {len(concatenated_prompt)} chars from {len(system_prompt_parts)} sections")
            else:
                self._log_error("No prompts available for system prompt configuration")

            # Configure server settings from config using direct property assignment
            mcp_config = self.config.get('mcp', {})
            if 'server_url' in mcp_config:
                self.mcp_client.server_url = mcp_config['server_url']
            if 'model' in mcp_config:
                self.mcp_client.model = mcp_config['model']
            if 'timeout' in mcp_config:
                self.mcp_client.timeout = mcp_config['timeout']

            # Add debug logging for what we're actually configuring
            self._log_debug(f"MCP client configured:")
            self._log_debug(f"  server_url: {self.mcp_client.server_url}")
            self._log_debug(f"  model: {self.mcp_client.model}")
            self._log_debug(f"  timeout: {getattr(self.mcp_client, 'timeout', 'not set')}")

        except Exception as e:
            self._log_error(f"MCP client configuration failed: {e}")

    def get_llm_queue_status(self) -> Dict[str, Any]:
        """Get current LLM queue status for debugging"""
        try:
            with self.state.llm_processing_lock:
                current_request = self.state.current_llm_request

            return {
                "queue_size": self.state.llm_queue.qsize(),
                "worker_running": (
                    self.state.llm_worker_thread is not None and
                    self.state.llm_worker_thread.is_alive()
                ),
                "current_request": {
                    "type": current_request.request_type.value if current_request else None,
                    "id": current_request.request_id if current_request else None,
                    "timestamp": current_request.timestamp if current_request else None
                } if current_request else None,
                "completed_requests": len(self.state.completed_requests),
                "failed_requests": len(self.state.failed_requests),
                "next_request_id": self.state.next_request_id
            }

        except Exception as e:
            self._log_error(f"Queue status check failed: {e}")
            return {"error": str(e)}

    def _add_queue_stats_to_system_stats(self, stats: Dict[str, Any]):
        """Add LLM queue statistics to system stats"""
        try:
            queue_status = self.get_llm_queue_status()
            stats["llm_queue"] = queue_status
            self._log_debug("Added LLM queue stats to system stats")
        except Exception as e:
            self._log_debug(f"Failed to add queue stats: {e}")
    
    def _setup_module_callbacks(self):
        """Set up orchestrator callbacks for modules that need cross-module communication"""
        try:
            # Memory manager needs orchestrator callback for condensation requests
            if self.memory_manager:
                self.memory_manager.set_orchestrator_callback(self._handle_memory_callback)

            # Semantic engine needs orchestrator callback for LLM requests
            if self.semantic_engine:
                self.semantic_engine.set_orchestrator_callback(self._handle_semantic_callback)

            # Momentum engine needs no callbacks (state tracking only)
            # UI controller callback already set in constructor

            self._log_debug("Module callbacks configured")

        except Exception as e:
            self._log_error(f"Callback setup failed: {e}")
    
    def _start_background_services(self):
        """Start background threads for periodic operations - UPDATED with LLM worker"""
        try:
            # Start LLM worker thread (NEW)
            self._start_llm_worker()

            # Start memory auto-save thread (existing)
            if self.memory_manager and hasattr(self.memory_manager, 'start_auto_save'):
                self.memory_manager.start_auto_save()
                self._log_debug("Memory auto-save thread started")

            # Start periodic analysis thread (existing)
            self.analysis_thread = threading.Thread(
                target=self._analysis_worker,
                daemon=True,
                name="AnalysisWorker"
            )
            self.analysis_thread.start()
            self.state.background_threads.append(self.analysis_thread)
            self._log_debug("Background analysis thread started")

        except Exception as e:
            self._log_error(f"Background service startup failed: {e}")

    def _start_llm_worker(self):
        """Start the LLM worker thread for serial processing"""
        try:
            self.state.llm_worker_thread = threading.Thread(
                target=self._llm_worker,
                daemon=True,
                name="LLMWorker"
            )
            self.state.llm_worker_thread.start()
            self.state.background_threads.append(self.state.llm_worker_thread)
            self._log_debug("LLM worker thread started")

        except Exception as e:
            self._log_error(f"LLM worker startup failed: {e}")

    def _llm_worker(self):
        """
        Serial LLM worker thread - processes one request at a time
        Ensures no concurrent LLM calls
        ENHANCED: Added detailed debug logging to trace queue issues
        """
        self._log_debug("LLM worker thread started")

        while not self.state.llm_worker_shutdown.is_set():
            try:
                # ENHANCED: Log queue status before each attempt
                queue_size = self.state.llm_queue.qsize()
                self._log_debug(f"LLM worker loop: queue_size={queue_size}, shutdown={self.state.llm_worker_shutdown.is_set()}")

                # Get next request from queue (blocks until available or timeout)
                try:
                    # ENHANCED: Log the queue get attempt
                    self._log_debug("LLM worker: attempting to get request from queue...")
                    priority, request = self.state.llm_queue.get(timeout=1.0)
                    self._log_debug(f"LLM worker: got request from queue, priority={priority}")
                except queue.Empty:
                    self._log_debug("LLM worker: queue empty, continuing...")
                    continue  # Check shutdown and try again

                if request is None:  # Shutdown signal
                    self._log_debug("LLM worker: received shutdown signal")
                    break

                self._log_debug(f"Processing LLM request: {request.request_type.value} (ID: {request.request_id})")

                # Set current request for tracking
                with self.state.llm_processing_lock:
                    self.state.current_llm_request = request

                # ENHANCED: Log before processing
                self._log_debug(f"About to process request type: {request.request_type}")

                # Process the request based on type
                try:
                    if request.request_type == LLMRequestType.USER_RESPONSE:
                        self._log_debug("Calling _process_user_response_request...")
                        self._process_user_response_request(request)
                    elif request.request_type == LLMRequestType.SEMANTIC_ANALYSIS:
                        self._log_debug("Calling _process_semantic_analysis_request...")
                        self._process_semantic_analysis_request(request)
                    elif request.request_type == LLMRequestType.CONDENSATION:
                        self._log_debug("Calling _process_condensation_request...")
                        self._process_condensation_request(request)
                    else:
                        self._log_error(f"Unknown LLM request type: {request.request_type}")

                    self._log_debug(f"Completed processing request: {request.request_id}")

                except Exception as e:
                    self._log_error(f"LLM request processing failed: {e}")
                    self._record_failed_request(request, str(e))

                finally:
                    # ENHANCED: Log cleanup
                    self._log_debug(f"Cleaning up request: {request.request_id}")

                    # Clear current request
                    with self.state.llm_processing_lock:
                        self.state.current_llm_request = None

                    # Mark queue task as done
                    self.state.llm_queue.task_done()
                    self._log_debug(f"Request cleanup complete: {request.request_id}")

            except Exception as e:
                self._log_error(f"LLM worker error: {e}")
                # Continue processing other requests

        self._log_debug("LLM worker thread stopped")

    def _process_user_response_request(self, request: LLMRequest):
        """Process a user response LLM request with enhanced debugging"""
        try:
            user_input = request.user_input
            context_data = request.context_data

            self._log_debug(f"Processing user response for: {user_input[:50]}...")

            # STEP 1: Check if resolution guidance should be added BEFORE making LLM request
            self._log_debug("Step 1: Checking resolution guidance...")
            resolution_guidance = self._check_resolution_guidance()
            self._log_debug(f"Resolution guidance result: {bool(resolution_guidance)}")

            # STEP 2: Make the actual LLM request (this is the serial bottleneck)
            self._log_debug("Step 2: Making LLM request...")
            llm_response = self._make_llm_request(user_input)
            self._log_debug(f"LLM request completed, success: {llm_response.get('success', False)}")

            if llm_response.get("success", False):
                # Store LLM response in memory
                response_text = llm_response.get("response", "")
                self._log_debug(f"Step 3: Storing response ({len(response_text)} chars)...")

                if self.memory_manager and response_text:
                    self.memory_manager.add_message(response_text, MessageType.ASSISTANT)
                    self.state.message_count += 1

                    # NEW: Add immediate semantic analysis for assistant message
                    if self.memory_manager.messages:
                        assistant_message = self.memory_manager.messages[-1]
                        self._perform_immediate_semantic_analysis(assistant_message)

                    # Force save of assistant response
                    if hasattr(self.memory_manager, '_auto_save'):
                        try:
                            self.memory_manager._auto_save()
                            self._log_debug("Assistant response saved successfully")
                        except Exception as save_error:
                            self._log_error(f"Failed to save assistant response: {save_error}")

                    self._log_debug("LLM response stored in memory")

                    # STEP 3: Update momentum engine with timeout protection
                    self._log_debug("Step 4: Updating momentum engine...")
                    if self.momentum_engine:
                        try:
                            self._log_debug("Calling momentum engine...")
                            momentum_result = self.momentum_engine.process_user_input(user_input, self.state.message_count)
                            self._log_debug(f"Momentum engine completed: pressure={momentum_result.get('pressure', 0.0)}")
                        except Exception as e:
                            self._log_error(f"Momentum engine processing failed: {e}")
                            # Continue processing even if momentum fails
                    else:
                        self._log_debug("No momentum engine available")

                    # Record successful completion
                    self._log_debug("Step 5: Recording successful completion...")
                    self._record_completed_request(request, {
                        "response": response_text,
                        "success": True,
                        "resolution_guidance_sent": bool(resolution_guidance)
                    })
            else:
                # Store error message
                error_msg = f"LLM Error: {llm_response.get('error', 'Unknown error')}"
                self._log_error(f"LLM request failed: {error_msg}")

                if self.memory_manager:
                    self.memory_manager.add_message(error_msg, MessageType.SYSTEM)
                    if hasattr(self.memory_manager, '_auto_save'):
                        try:
                            self.memory_manager._auto_save()
                        except Exception:
                            pass

                self._record_failed_request(request, error_msg)

            self._log_debug("User response processing completed")

        except Exception as e:
            self._log_error(f"User response request failed: {e}")
            self._record_failed_request(request, str(e))

    def debug_queue_status(self) -> Dict[str, Any]:
        """Debug method to check queue status"""
        try:
            with self.state.llm_processing_lock:
                current_request = self.state.current_llm_request

            return {
                "queue_size": self.state.llm_queue.qsize(),
                "worker_alive": self.state.llm_worker_thread.is_alive() if self.state.llm_worker_thread else False,
                "worker_shutdown_set": self.state.llm_worker_shutdown.is_set(),
                "current_request_id": current_request.request_id if current_request else None,
                "completed_count": len(self.state.completed_requests),
                "failed_count": len(self.state.failed_requests)
            }
        except Exception as e:
            return {"error": str(e)}

    def _process_semantic_analysis_request(self, request: LLMRequest):
        """Process a semantic analysis LLM request"""
        try:
            self._log_debug("Processing semantic analysis request")

            # Extract semantic analysis data
            analysis_data = request.context_data
            content = analysis_data.get("content", "")
            message_id = analysis_data.get("message_id", "")

            # Build semantic analysis prompt
            prompt = f"""
Analyze this RPG message for semantic importance:

Message: "{content}"

Provide analysis in JSON format:
{{
    "importance_score": 0.0-1.0,
    "categories": ["story_critical", "character_focused", "relationship_dynamics", "emotional_significance", "world_building", "standard"],
    "reasoning": "Brief explanation"
}}
"""

            # Make LLM request for semantic analysis
            llm_response = self._make_llm_request_for_analysis(prompt, content)

            if llm_response.get("success", False):
                # Parse response and update message category
                response_text = llm_response.get("response", "")
                category = self._parse_semantic_response(response_text)

                if category and self.memory_manager:
                    self.memory_manager.update_message_category(message_id, category)
                    self._log_debug(f"Updated message {message_id} category to {category}")

                self._record_completed_request(request, {
                    "category": category,
                    "success": True
                })
            else:
                self._record_failed_request(request, llm_response.get("error", "Analysis failed"))

        except Exception as e:
            self._log_error(f"Semantic analysis request failed: {e}")
            self._record_failed_request(request, str(e))

    def _process_condensation_request(self, request: LLMRequest):
        """Process a condensation LLM request"""
        try:
            self._log_debug("Processing condensation request")

            # For now, implement basic condensation without LLM
            # TODO: Implement full LLM-based condensation
            condensed_content = "[Condensed summary of narrative progression]"

            self._record_completed_request(request, {
                "condensed_content": condensed_content,
                "success": True
            })

        except Exception as e:
            self._log_error(f"Condensation request failed: {e}")
            self._record_failed_request(request, str(e))

    def _record_completed_request(self, request: LLMRequest, result: Dict[str, Any]):
        """Record a successfully completed LLM request"""
        try:
            self.state.completed_requests[request.request_id] = {
                "request_type": request.request_type.value,
                "timestamp": time.time(),
                "result": result,
                "processing_time": time.time() - request.timestamp
            }
            self._log_debug(f"Recorded completed request: {request.request_id}")

        except Exception as e:
            self._log_error(f"Failed to record completed request: {e}")

    def _record_failed_request(self, request: LLMRequest, error_message: str):
        """Record a failed LLM request"""
        try:
            self.state.failed_requests[request.request_id] = {
                "request_type": request.request_type.value,
                "timestamp": time.time(),
                "error": error_message,
                "processing_time": time.time() - request.timestamp
            }
            self._log_error(f"Recorded failed request: {request.request_id} - {error_message}")

        except Exception as e:
            self._log_error(f"Failed to record failed request: {e}")

    def _make_llm_request_for_analysis(self, prompt: str, content: str) -> Dict[str, Any]:
        """Make simplified LLM request for analysis purposes"""
        try:
            if not self.mcp_client:
                return {"success": False, "error": "MCP client not available"}

            # Create simplified context for analysis
            response = self.mcp_client.send_message(
                user_input=content,
                conversation_history=[],  # No history for analysis
                story_context=prompt      # Use prompt as context
            )

            return {"success": True, "response": response}

        except Exception as e:
            self._log_error(f"Analysis LLM request failed: {e}")
            return {"success": False, "error": str(e)}

    def _parse_semantic_response(self, response: str) -> str:
        """Parse semantic analysis response to extract category"""
        try:
            import json
            import re

            # Try to find JSON in response
            json_match = re.search(r'\{[^{}]*\}', response, re.DOTALL)
            if json_match:
                data = json.loads(json_match.group())
                categories = data.get("categories", ["standard"])
                if categories and isinstance(categories, list):
                    return categories[0]  # Return first category

            # Fallback: look for category names in text
            category_keywords = {
                "story_critical": ["critical", "important", "key", "vital"],
                "character_focused": ["character", "personality", "development"],
                "relationship_dynamics": ["relationship", "social", "interaction"],
                "emotional_significance": ["emotion", "feeling", "heart", "dramatic"],
                "world_building": ["world", "setting", "environment", "lore"]
            }

            response_lower = response.lower()
            for category, keywords in category_keywords.items():
                if any(keyword in response_lower for keyword in keywords):
                    return category

            return "standard"  # Default fallback

        except Exception as e:
            self._log_debug(f"Semantic response parsing failed: {e}")
            return "standard"
    
    def _analysis_worker(self):
        """Background worker for periodic analysis operations"""
        self._log_debug("Analysis worker thread started")
        
        while not self.analysis_shutdown_event.is_set():
            try:
                # Check if analysis is needed
                if (self.state.message_count - self.state.last_analysis_count >= self.ANALYSIS_INTERVAL 
                    and not self.state.analysis_in_progress 
                    and self.state.phase == OrchestrationPhase.ACTIVE):
                    
                    self._trigger_periodic_analysis()
                
                # Sleep for short interval to avoid busy waiting
                self.analysis_shutdown_event.wait(timeout=5.0)
                
            except Exception as e:
                self._log_error(f"Analysis worker error: {e}")
                time.sleep(1.0)  # Prevent rapid error loops
        
        self._log_debug("Analysis worker thread stopped")

# Chunk 3/5 - orch.py - Core Processing Methods (Debug Logger Fix)

    def run(self) -> int:
        """
        Main orchestrator run loop - initializes and coordinates all modules
        Returns exit code for main.py
        """
        try:
            self._log_debug("Starting orchestrator run sequence")

            # Initialize all modules
            if not self.initialize_modules():
                self._log_error("Module initialization failed")
                return 1

            # Transfer control to UI controller
            if not self.ui_controller:
                self._log_error("UI controller not available")
                return 1

            self._log_debug("Transferring control to UI controller")
            exit_code = self.ui_controller.run()

            # Cleanup after UI exits
            self._shutdown_modules()

            self._log_debug(f"Orchestrator run completed with exit code: {exit_code}")
            return exit_code

        except Exception as e:
            self._log_error(f"Orchestrator run failed: {e}")
            return 1
    
    def _handle_ui_callback(self, action: str, data: Dict[str, Any]) -> Any:
        """
        Handle callbacks from UI controller
        UPDATED: Added support for stateless UI operations
        """
        try:
            # Only log non-routine callbacks to reduce spam
            if action not in ["get_messages", "get_display_status"]:
                self._log_debug(f"Processing UI callback: {action}")

            if action == "user_input":
                return self._process_user_input(data)
            elif action == "get_messages":
                return self._get_message_history(data)
            elif action == "add_system_message":  # NEW
                return self._add_system_message(data)
            elif action == "clear_memory":
                return self._clear_memory()
            elif action == "queue_debug":
                return self._handle_queue_debug(data)
            elif action == "get_stats":
                return self._get_system_stats()
            elif action == "analyze_now" or action == "force_analysis":
                return self._trigger_immediate_analysis()
            elif action == "shutdown":
                return self._handle_shutdown()
            else:
                self._log_error(f"Unknown UI callback action: {action}")
                return {"success": False, "error": f"Unknown action: {action}"}

        except Exception as e:
            self._log_error(f"UI callback handling failed for {action}: {e}")
            return {"success": False, "error": str(e)}

    def _add_system_message(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Add system message to memory
        NEW: Support for stateless UI to add messages through orchestrator
        """
        try:
            content = data.get("content", "")
            message_type = data.get("message_type", "system")

            if not content:
                return {"success": False, "error": "Empty message content"}

            # Convert message type string to enum
            if message_type == "system":
                msg_type_enum = MessageType.SYSTEM
            elif message_type == "user":
                msg_type_enum = MessageType.USER
            elif message_type == "assistant":
                msg_type_enum = MessageType.ASSISTANT
            else:
                msg_type_enum = MessageType.SYSTEM  # Default fallback

            # Store message in memory
            if self.memory_manager:
                self.memory_manager.add_message(content, msg_type_enum)
                self.state.message_count += 1

                # CRITICAL: Force immediate save to ensure message is available for get_messages
                if hasattr(self.memory_manager, '_auto_save'):
                    self.memory_manager._auto_save()

                self._log_debug(f"Added {message_type} message: {content[:50]}...")

            return {"success": True, "message_count": self.state.message_count}

        except Exception as e:
            self._log_error(f"Add system message failed: {e}")
            return {"success": False, "error": str(e)}

    def _process_user_input(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Process user input with serial LLM queue - FIXED for immediate echo
        """
        try:
            user_input = data.get("input", "").strip()
            if not user_input:
                return {"success": False, "error": "Empty input"}

            self._log_debug("Processing user input via queue system")

            # 1. Validate input through semantic engine (quick operation)
            if self.semantic_engine:
                validation_result = self.semantic_engine.validate_input(user_input)
                if not validation_result.get("valid", True):
                    return {"success": False, "error": validation_result.get("error", "Input validation failed")}

            # 2. Store user message in memory IMMEDIATELY for echo
            if self.memory_manager:
                self.memory_manager.add_message(user_input, MessageType.USER)
                self.state.message_count += 1

                # NEW: Add immediate semantic analysis for user message
                if self.memory_manager.messages:
                    user_message = self.memory_manager.messages[-1]
                    self._perform_immediate_semantic_analysis(user_message)

                # CRITICAL: Force immediate save for echo availability
                if hasattr(self.memory_manager, '_auto_save'):
                    try:
                        self.memory_manager._auto_save()
                        self._log_debug("User message saved and available for echo")
                    except Exception as save_error:
                        self._log_error(f"Failed to save user message: {save_error}")

            # 3. Queue LLM processing instead of spawning background thread
            request_id = self._generate_request_id()
            llm_request = LLMRequest(
                request_type=LLMRequestType.USER_RESPONSE,
                user_input=user_input,
                context_data={"original_data": data},
                request_id=request_id,
                timestamp=time.time(),
                priority=1  # High priority for user responses
            )

            # Add to queue for serial processing
            self.state.llm_queue.put((llm_request.priority, llm_request))
            self._log_debug(f"Queued LLM request {request_id} for processing")

            # CRITICAL FIX: Check if worker thread died and restart if needed
            if not self.state.llm_worker_thread or not self.state.llm_worker_thread.is_alive():
                self._log_error("CRITICAL: LLM worker thread died! Restarting...")
                self._start_llm_worker()
                time.sleep(0.1)  # Give new thread a moment to start

            # 4. Return success IMMEDIATELY - user message is available for echo
            return {
                "success": True,
                "message": "User input processed",
                "message_count": self.state.message_count,
                "request_id": request_id,
                "queued_for_processing": True
            }

        except Exception as e:
            self._log_error(f"User input processing failed: {e}")
            return {"success": False, "error": str(e)}

    def _generate_request_id(self) -> str:
        """Generate unique request ID"""
        request_id = f"req_{self.state.next_request_id:04d}_{int(time.time() * 1000) % 10000}"
        self.state.next_request_id += 1
        return request_id

    def _queue_semantic_analysis(self, message_id: str, content: str, msg_type: str):
        """Queue semantic analysis request for background processing"""
        try:
            request_id = self._generate_request_id()

            semantic_request = LLMRequest(
                request_type=LLMRequestType.SEMANTIC_ANALYSIS,
                user_input=content,
                context_data={
                    "message_id": message_id,
                    "content": content,
                    "msg_type": msg_type
                },
                request_id=request_id,
                timestamp=time.time(),
                priority=3  # Lower priority than user responses
            )

            self.state.llm_queue.put((semantic_request.priority, semantic_request))
            self._log_debug(f"Queued semantic analysis {request_id}")

            return request_id

        except Exception as e:
            self._log_error(f"Failed to queue semantic analysis: {e}")
            return None
    
    def _make_llm_request(self, user_input: str) -> Dict[str, Any]:
        """
        Make LLM request - ONLY function that calls mcp.py
        FIXED: Handle both string and dictionary responses from MCP client
        """
        try:
            if not self.mcp_client:
                return {"success": False, "error": "MCP client not available"}

            self._log_debug("Making LLM request")

            # Get conversation context from memory - FIXED method name
            context = []
            if self.memory_manager:
                # Use get_conversation_for_mcp() instead of get_conversation_context()
                if hasattr(self.memory_manager, 'get_conversation_for_mcp'):
                    context = self.memory_manager.get_conversation_for_mcp()
                elif hasattr(self.memory_manager, 'get_messages'):
                    # Fallback to get_messages if available
                    messages = self.memory_manager.get_messages()
                    # Convert Message objects to MCP format
                    context = []
                    for msg in messages:
                        context.append({
                            "role": msg.message_type.value.lower(),  # Convert enum to string
                            "content": msg.content
                        })

            # Get current momentum state for context - FIXED: Handle missing methods gracefully
            momentum_data = {}
            if self.momentum_engine:
                try:
                    if hasattr(self.momentum_engine, 'get_current_state'):
                        momentum_data = self.momentum_engine.get_current_state()
                    elif hasattr(self.momentum_engine, 'get_state'):
                        momentum_data = self.momentum_engine.get_state()
                    else:
                        # Fallback: provide empty momentum context
                        momentum_data = {}
                        self._log_debug("Momentum engine has no state retrieval method")
                except Exception as e:
                    self._log_debug(f"Failed to get momentum state: {e}")
                    momentum_data = {}

            # FIXED: Use correct MCP client parameter names from mcp.py
            raw_response = None
            try:
                if hasattr(self.mcp_client, 'send_message'):
                    # Get story context from momentum engine
                    story_context = None
                    if momentum_data:
                        # Convert momentum data to story context string
                        story_context = f"Current pressure: {momentum_data.get('narrative_pressure', 0.0)}, "
                        story_context += f"Story arc: {momentum_data.get('story_arc', 'unknown')}, "
                        story_context += f"Manifestation: {momentum_data.get('manifestation_type', 'exploration')}"

                    # Call with correct parameter names matching mcp.py
                    raw_response = self.mcp_client.send_message(
                        user_input=user_input,
                        conversation_history=context,  # ✅ Correct parameter name
                        story_context=story_context    # ✅ Correct parameter name
                    )
                else:
                    return {"success": False, "error": "MCP client has no send_message method"}

            except Exception as method_error:
                self._log_error(f"MCP method call failed: {method_error}")
                return {"success": False, "error": f"MCP call error: {str(method_error)}"}

            # FIXED: Handle both string and dictionary responses from MCP client
            if raw_response is None:
                return {"success": False, "error": "No response from MCP client"}

            # Convert response to standard format
            if isinstance(raw_response, str):
                # MCP client returned a string - assume it's the LLM response content
                response = {
                    "success": True,
                    "response": raw_response,
                    "error": None
                }
                self._log_debug("LLM request completed successfully (string response)")
            elif isinstance(raw_response, dict):
                # MCP client returned a dictionary - use as-is or normalize
                response = raw_response
                if response.get("success", False):
                    self._log_debug("LLM request completed successfully (dict response)")
                else:
                    self._log_error(f"LLM request failed: {response.get('error', 'Unknown error')}")
            else:
                # Unexpected response type
                self._log_error(f"Unexpected MCP response type: {type(raw_response)}")
                return {"success": False, "error": f"Invalid response type: {type(raw_response)}"}

            return response

        except Exception as e:
            self._log_error(f"LLM request error: {e}")
            return {"success": False, "error": str(e)}
    
    def _get_message_history(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Get message history from memory manager
        FIXED: Handle Message objects correctly and convert to UI format
        """
        try:
            if not self.memory_manager:
                return {"success": False, "error": "Memory manager not available"}

            # Get parameters from data
            limit = data.get("limit", 50)

            messages = []

            # Try to get messages from memory manager
            if hasattr(self.memory_manager, 'get_messages'):
                # Get Message objects
                message_objects = self.memory_manager.get_messages(limit)

                # Convert Message objects to UI format
                for msg in message_objects:
                    # Message objects have attributes, not dictionary keys
                    messages.append({
                        "content": msg.content,
                        "type": msg.message_type.value.lower(),  # Convert enum to string
                        "timestamp": msg.timestamp,
                        "id": getattr(msg, 'id', None)
                    })

            elif hasattr(self.memory_manager, 'get_conversation_for_mcp'):
                # Get MCP format and convert to UI format
                mcp_messages = self.memory_manager.get_conversation_for_mcp()

                for msg in mcp_messages[-limit:]:
                    messages.append({
                        "content": msg.get("content", ""),
                        "type": msg.get("role", "user"),
                        "timestamp": None
                    })

            return {
                "success": True,
                "messages": messages,
                "total_count": len(messages)
            }

        except Exception as e:
            self._log_error(f"Message history retrieval failed: {e}")
            return {"success": False, "error": str(e)}

# Chunk 4/5 - orch.py - Analysis and Utility Methods (Debug Logger Fix)

    def _trigger_periodic_analysis(self):
        """
        Trigger comprehensive analysis every 15 messages
        FIXED: Use correct memory manager method name
        """
        if self.state.analysis_in_progress:
            self._log_debug("Analysis already in progress, skipping")
            return

        try:
            self.state.analysis_in_progress = True
            self.state.phase = OrchestrationPhase.ANALYZING

            self._log_debug("Starting periodic analysis")

            # Get conversation context for analysis - FIXED method name
            analysis_context = []
            if self.memory_manager:
                if hasattr(self.memory_manager, 'get_conversation_for_mcp'):
                    analysis_context = self.memory_manager.get_conversation_for_mcp()
                elif hasattr(self.memory_manager, 'get_messages'):
                    # Convert Message objects to analysis format
                    messages = self.memory_manager.get_messages()
                    for msg in messages:
                        analysis_context.append({
                            "id": getattr(msg, 'id', None),
                            "content": msg.content,
                            "role": msg.message_type.value.lower(),
                            "timestamp": msg.timestamp
                        })

            # Run semantic analysis
            semantic_results = {}
            if self.semantic_engine and analysis_context:
                semantic_results = self.semantic_engine.analyze_conversation(analysis_context)
                self._log_debug("Semantic analysis completed")

            # Run momentum analysis
            momentum_results = {}
            if self.momentum_engine and analysis_context:
                momentum_results = self.momentum_engine.analyze_momentum(analysis_context)
                self._log_debug("Momentum analysis completed")

            # Store analysis results
            self.state.analysis_results = {
                "semantic": semantic_results,
                "momentum": momentum_results,
                "timestamp": time.time(),
                "message_count": self.state.message_count
            }

            # Update analysis tracking
            self.state.last_analysis_count = self.state.message_count

            self._log_debug("Periodic analysis completed successfully")

        except Exception as e:
            self._log_error(f"Periodic analysis failed: {e}")
        finally:
            self.state.analysis_in_progress = False
            self.state.phase = OrchestrationPhase.ACTIVE
    
    def _trigger_immediate_analysis(self) -> Dict[str, Any]:
        """Trigger immediate analysis requested by user"""
        try:
            if self.state.analysis_in_progress:
                return {"success": False, "error": "Analysis already in progress"}
            
            self._log_debug("Starting immediate analysis")
            
            # Run analysis in current thread since user requested it
            self._trigger_periodic_analysis()
            
            return {
                "success": True,
                "results": self.state.analysis_results
            }
            
        except Exception as e:
            self._log_error(f"Immediate analysis failed: {e}")
            return {"success": False, "error": str(e)}
    
    def _clear_memory(self) -> Dict[str, Any]:
        """
        Clear conversation memory - FIXED reset calls
        """
        try:
            if not self.memory_manager:
                return {"success": False, "error": "Memory manager not available"}

            self._log_debug("Clearing conversation memory")

            # Clear memory using correct method
            self.memory_manager.reset_state()  # Now uses our new method

            # Reset orchestrator counters
            self.state.message_count = 0
            self.state.last_analysis_count = 0
            self.state.analysis_results = {}

            # Reset momentum engine using correct method
            if self.momentum_engine:
                self.momentum_engine.reset_state()  # Now uses our new method
                self._log_debug("Momentum engine state reset")

            self._log_debug("Memory cleared successfully")

            return {"success": True}

        except Exception as e:
            self._log_error(f"Memory clear failed: {e}")
            return {"success": False, "error": str(e)}
    
    def _get_system_stats(self) -> Dict[str, Any]:
        """Get current system statistics with resolution monitoring"""
        try:
            stats = {
                "message_count": self.state.message_count,
                "last_analysis_count": self.state.last_analysis_count,
                "analysis_in_progress": self.state.analysis_in_progress,
                "phase": self.state.phase.value,
                "startup_complete": self.state.startup_complete
            }

            # Add memory stats
            if self.memory_manager:
                try:
                    memory_stats = self.memory_manager.get_memory_stats()
                    stats["memory"] = memory_stats
                    self._log_debug("Memory stats retrieved successfully")
                except Exception as e:
                    self._log_debug(f"Memory stats unavailable: {e}")
                    stats["memory"] = {"error": "stats unavailable"}

            # Add momentum stats with resolution state
            if self.momentum_engine:
                try:
                    momentum_stats = self.momentum_engine.get_pressure_stats()

                    # Add resolution monitoring info
                    resolution_trigger = self.momentum_engine.check_resolution_trigger()
                    momentum_stats["resolution_state"] = {
                        "guidance_needed": bool(resolution_trigger),
                        "trigger_type": resolution_trigger.get("trigger_type") if resolution_trigger else None,
                        "guidance_sent": getattr(self.momentum_engine, '_resolution_guidance_sent', False)
                    }

                    stats["momentum"] = momentum_stats
                    self._log_debug("Momentum stats with resolution state retrieved")
                except Exception as e:
                    self._log_debug(f"Momentum stats unavailable: {e}")
                    stats["momentum"] = {"error": "stats unavailable"}

            # Add MCP stats
            if self.mcp_client:
                try:
                    if hasattr(self.mcp_client, 'get_server_info'):
                        mcp_stats = self.mcp_client.get_server_info()
                        stats["mcp"] = mcp_stats
                    else:
                        stats["mcp"] = {
                            "available": True,
                            "server_url": getattr(self.mcp_client, 'server_url', 'unknown'),
                            "model": getattr(self.mcp_client, 'model', 'unknown')
                        }
                    self._log_debug("MCP stats retrieved successfully")
                except Exception as e:
                    self._log_debug(f"MCP stats unavailable: {e}")
                    stats["mcp"] = {"error": "stats unavailable"}

            # Add LLM queue stats
            self._add_queue_stats_to_system_stats(stats)

            return {"success": True, "stats": stats}

        except Exception as e:
            self._log_error(f"Stats retrieval failed: {e}")
            return {"success": False, "error": str(e)}

    def _handle_queue_debug(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """Handle queue debugging commands"""
        try:
            command = data.get("command", "")

            if command == "status":
                return {
                    "success": True,
                    "queue_status": self.get_llm_queue_status(),
                    "recent_completed": list(self.state.completed_requests.keys())[-5:],
                    "recent_failed": list(self.state.failed_requests.keys())[-5:]
                }
            elif command == "clear_history":
                # Clear old request history to prevent memory buildup
                self.state.completed_requests.clear()
                self.state.failed_requests.clear()
                return {"success": True, "message": "Request history cleared"}
            elif command == "worker_restart":
                # Emergency worker restart (for debugging only)
                if self.state.llm_worker_thread:
                    self.state.llm_worker_shutdown.set()
                    self.state.llm_worker_thread.join(timeout=5.0)
                self.state.llm_worker_shutdown.clear()
                self._start_llm_worker()
                return {"success": True, "message": "LLM worker restarted"}
            else:
                return {"success": False, "error": f"Unknown queue debug command: {command}"}

        except Exception as e:
            self._log_error(f"Queue debug failed: {e}")
            return {"success": False, "error": str(e)}
    
    def _handle_shutdown(self) -> Dict[str, Any]:
        """Handle shutdown request from UI"""
        try:
            self._log_debug("Shutdown requested")
            self.state.shutdown_requested = True
            return {"success": True}
            
        except Exception as e:
            self._log_error(f"Shutdown handling failed: {e}")
            return {"success": False, "error": str(e)}
    
    def _shutdown_modules(self):
        """Clean shutdown of all modules and background threads - UPDATED with LLM cleanup"""
        try:
            self._log_debug("Starting module shutdown")
            self.state.phase = OrchestrationPhase.SHUTTING_DOWN

            # Stop LLM worker thread (NEW)
            if self.state.llm_worker_thread and self.state.llm_worker_thread.is_alive():
                self.state.llm_worker_shutdown.set()
                # Send shutdown signal to queue
                self.state.llm_queue.put((0, None))  # High priority shutdown signal
                self.state.llm_worker_thread.join(timeout=5.0)
                self._log_debug("LLM worker thread stopped")

            # Stop background analysis thread (existing)
            if self.analysis_thread and self.analysis_thread.is_alive():
                self.analysis_shutdown_event.set()
                self.analysis_thread.join(timeout=5.0)
                self._log_debug("Analysis thread stopped")

            # Shutdown modules (existing code)
            if self.memory_manager and hasattr(self.memory_manager, 'shutdown'):
                self.memory_manager.shutdown()
                self._log_debug("Memory manager shutdown")

            if self.momentum_engine and hasattr(self.momentum_engine, 'shutdown'):
                self.momentum_engine.shutdown()
                self._log_debug("Momentum engine shutdown")

            if self.semantic_engine and hasattr(self.semantic_engine, 'shutdown'):
                self.semantic_engine.shutdown()
                self._log_debug("Semantic engine shutdown")

            if self.mcp_client and hasattr(self.mcp_client, 'shutdown'):
                self.mcp_client.shutdown()
                self._log_debug("MCP client shutdown")

            # Wait for remaining background threads
            for thread in self.state.background_threads:
                if thread.is_alive():
                    thread.join(timeout=2.0)

            self._log_debug("All modules shutdown completed")

        except Exception as e:
            self._log_error(f"Module shutdown failed: {e}")

# Chunk 5/5 - orch.py - Debug Logging Helper Methods (Debug Logger Fix)

    def _log_debug(self, message: str):
        """
        Standardized debug logging with null safety
        Uses method pattern: self.debug_logger.debug(message, "ORCHESTRATOR")
        """
        if self.debug_logger:
            self.debug_logger.debug(message, "ORCHESTRATOR")
    
    def _log_error(self, message: str):
        """
        Standardized error logging with null safety
        Uses method pattern: self.debug_logger.error(message, "ORCHESTRATOR")
        """
        if self.debug_logger:
            self.debug_logger.error(message, "ORCHESTRATOR")
    
    def _log_system(self, message: str):
        """
        Standardized system logging with null safety
        Uses method pattern: self.debug_logger.system(message)
        """
        if self.debug_logger:
            self.debug_logger.system(message)

    def _handle_memory_callback(self, request: Dict[str, Any]) -> Dict[str, Any]:
        """Handle memory manager callback requests (condensation, etc.)"""
        try:
            request_type = request.get("request_type", "")

            if request_type == "condensation":
                return self._handle_condensation_request(request)
            else:
                return {"success": False, "error": f"Unknown memory request: {request_type}"}

        except Exception as e:
            self._log_error(f"Memory callback failed: {e}")
            return {"success": False, "error": str(e)}

    def _handle_semantic_callback(self, request) -> Optional[Any]:
        """Handle semantic analysis callback requests (LLM analysis)"""
        try:
            # Convert semantic request to LLM request
            analysis_type = request.analysis_type
            context_data = request.context_data

            if analysis_type == "categorization":
                # Use the analysis prompt from context_data
                prompt = context_data.get("analysis_prompt", "")
                content = context_data.get("content", "")

                # Make LLM request for semantic analysis
                llm_result = self._make_llm_request_for_analysis(prompt, content)

                from sem import SemanticAnalysisResult
                if llm_result.get("success", False):
                    return SemanticAnalysisResult(
                        success=True,
                        analysis_type=analysis_type,
                        data={"response": llm_result.get("response", "")},
                        confidence=0.8
                    )
                else:
                    return SemanticAnalysisResult(
                        success=False,
                        analysis_type=analysis_type,
                        data={},
                        error_message=llm_result.get("error", "LLM request failed")
                    )

            return None

        except Exception as e:
            self._log_error(f"Semantic callback failed: {e}")
            return None

    def _make_llm_request_for_analysis(self, prompt: str, content: str) -> Dict[str, Any]:
        """Make simplified LLM request for analysis purposes"""
        try:
            if not self.mcp_client:
                return {"success": False, "error": "MCP client not available"}

            # Create a simplified message for analysis
            analysis_messages = [
                {"role": "system", "content": prompt},
                {"role": "user", "content": content}
            ]

            # Use MCP client directly for analysis
            response = self.mcp_client.send_message(
                user_input=content,
                conversation_history=[],  # No history for analysis
                story_context=prompt      # Use prompt as context
            )

            return {"success": True, "response": response}

        except Exception as e:
            self._log_error(f"Analysis LLM request failed: {e}")
            return {"success": False, "error": str(e)}

    def _handle_condensation_request(self, request: Dict[str, Any]) -> Dict[str, Any]:
        """Handle memory condensation request"""
        try:
            self._log_debug("Processing condensation request")

            # Get condensation candidates from memory manager
            candidates = request.get("candidates", [])
            memory_stats = request.get("memory_stats", {})

            if not candidates:
                return {"success": False, "error": "No condensation candidates provided"}

            # For now, implement basic condensation
            # TODO: Implement full semantic-aware condensation
            condensed_content = f"[Condensed summary of {len(candidates)} messages from narrative progression]"

            # Replace messages with condensed version
            if self.memory_manager:
                success = self.memory_manager.replace_messages_with_condensed(
                    message_ids=candidates[:len(candidates)//2],  # Condense first half
                    condensed_content=condensed_content
                )

                return {"success": success, "condensed_count": len(candidates)//2}

            return {"success": False, "error": "Memory manager not available"}

        except Exception as e:
            self._log_error(f"Condensation request failed: {e}")
            return {"success": False, "error": str(e)}

    def _check_resolution_guidance(self) -> Optional[str]:
        """
        Check if momentum engine suggests resolution guidance
        Returns system prompt if resolution guidance should be sent
        """
        if not self.momentum_engine:
            return None

        try:
            resolution_trigger = self.momentum_engine.check_resolution_trigger()

            if resolution_trigger:
                trigger_type = resolution_trigger.get("trigger_type", "unknown")
                guidance_prompt = resolution_trigger.get("guidance_prompt", "")
                force_conclusion = resolution_trigger.get("force_conclusion", False)

                self._log_debug(f"Resolution guidance triggered: {trigger_type}")

                # Add resolution guidance as system message
                if guidance_prompt:
                    self._add_resolution_guidance(guidance_prompt, force_conclusion)
                    return guidance_prompt

            return None

        except Exception as e:
            self._log_error(f"Resolution guidance check failed: {e}")
            return None

    def _add_resolution_guidance(self, guidance_prompt: str, force_conclusion: bool = False):
        """
        Add resolution guidance as system message
        """
        try:
            if self.memory_manager:
                # Create system message with resolution guidance
                priority_marker = "[CRITICAL] " if force_conclusion else "[GUIDANCE] "
                system_message = f"{priority_marker}{guidance_prompt}"

                self.memory_manager.add_message(system_message, MessageType.SYSTEM)

                # Force immediate save to ensure guidance is available for next LLM request
                if hasattr(self.memory_manager, '_auto_save'):
                    self.memory_manager._auto_save()

                self._log_debug("Resolution guidance added to conversation context")

        except Exception as e:
            self._log_error(f"Failed to add resolution guidance: {e}")

    def _perform_immediate_semantic_analysis(self, message) -> None:
        """
        Perform immediate pattern-based semantic analysis
        Called synchronously after each message is stored
        """
        try:
            content_lower = message.content.lower()

            # Story critical patterns - highest importance
            if any(phrase in content_lower for phrase in [
                "main quest", "primary objective", "critical mission", "prophecy",
                "fate of", "destiny", "chosen one", "final battle",
                "world ending", "save the kingdom", "last hope", "ancient evil",
                "bandit leader", "quest"  # Added based on your conversation
            ]):
                message.content_category = "story_critical"
                message.importance_score = 0.9

            # Character focused patterns
            elif any(phrase in content_lower for phrase in [
                "my name is", "i am called", "backstory", "my past",
                "character development", "personality", "my motivation",
                "why i", "i believe", "my goal", "my family", "i grew up",
                "i will play", "half-elf", "rogue named", "koryn"  # Added from your intro
            ]):
                message.content_category = "character_focused"
                message.importance_score = 0.8

            # Relationship dynamics patterns
            elif any(phrase in content_lower for phrase in [
                "trust", "betray", "alliance", "friendship", "romance",
                "rival", "enemy turned", "companion", "loyalty",
                "working together", "conflict between", "i care about",
                "kael", "lyria", "companions"  # Added your companions
            ]):
                message.content_category = "relationship_dynamics"
                message.importance_score = 0.7

            # Emotional significance patterns
            elif any(phrase in content_lower for phrase in [
                "i feel", "makes me", "emotional", "heart breaks",
                "fear overwhelms", "love", "hate", "joy", "sorrow",
                "triumph", "devastating", "overwhelming", "tears",
                "afraid", "scared", "terrified", "anxious", "worried"
            ]):
                message.content_category = "emotional_significance"
                message.importance_score = 0.6

            # World building patterns
            elif any(phrase in content_lower for phrase in [
                "history of", "legend says", "long ago", "ancient",
                "culture", "tradition", "geography", "politics",
                "economy", "religion", "magic system", "technology",
                "ashen wastes", "watchtower", "ruins"  # Added from your story
            ]):
                message.content_category = "world_building"
                message.importance_score = 0.5

            # Action/tension patterns that might be elevated
            elif any(phrase in content_lower for phrase in [
                "draw", "weapon", "rapier", "sword", "attack", "fight",
                "danger", "threat", "enemy", "presence", "watching",
                "ground trembles", "earth shudders", "ancient", "stirs"
            ]):
                message.content_category = "emotional_significance"  # Tension is emotionally significant
                message.importance_score = 0.6

            # Default standard category
            else:
                message.content_category = "standard"
                message.importance_score = 0.4

            # Add semantic metadata
            message.semantic_metadata = {
                "analysis_type": "pattern",
                "analyzed_at": time.time(),
                "content_length": len(message.content),
                "token_count": message.token_estimate,
                "analysis_version": "1.0"
            }

            self._log_debug(f"Semantic analysis: {message.id[:8]} -> {message.content_category} (score: {message.importance_score})")

        except Exception as e:
            self._log_error(f"Immediate semantic analysis failed for message {getattr(message, 'id', 'unknown')}: {e}")
            # Set safe defaults on error
            message.content_category = "standard"
            message.importance_score = 0.4
            message.semantic_metadata = {"analysis_type": "error", "error": str(e)}

# =============================================================================
# MODULE EXPORTS
# =============================================================================

__all__ = [
    'Orchestrator',
    'OrchestrationPhase', 
    'OrchestrationState'
]
