GENAI.TXT - AI ANALYSIS REFERENCE FOR AURORA RPG CLIENT
================================================================

PROJECT SCOPE:
Terminal-based RPG storytelling client leveraging Large Language Model capabilities through MCP (Model Control Protocol). Features innovative Story Momentum Engine and intelligent memory management with integrated prompt system architecture.

MODULE ARCHITECTURE:
main.py - Application coordination, prompt management, and lifecycle control
nci.py - Ncurses interface with integrated prompt system
mcp.py - HTTP client for MCP/Ollama server communication  
emm.py - Enhanced Memory Manager with LLM-powered semantic condensation
sme.py - Story Momentum Engine for narrative pressure management

CRITICAL INTERCONNECTS:
main.py → nci.py: Creates CursesInterface with loaded prompt configuration
nci.py → mcp.py: Sends integrated system messages via MCPClient methods
nci.py → emm.py: Stores/retrieves messages via EnhancedMemoryManager
nci.py → sme.py: Updates narrative pressure via StoryMomentumEngine.process_user_input()
mcp.py ← sme.py: Receives story context via get_story_context()
mcp.py ← emm.py: Receives conversation history via get_conversation_for_mcp()

DATA FLOW:
1. Prompts loaded and optimized in main.py during startup
2. User input captured in nci.py with integrated prompt system
3. Message stored in emm.py with automatic semantic analysis
4. Narrative pressure updated in sme.py based on input patterns
5. System messages built from loaded prompts + story context + conversation history
6. MCP request sent with complete message chain, response received
7. Response displayed in nci.py interface with memory storage

CONFIGURATION SYSTEM:
- Hardcoded configuration values in ApplicationConfig class
- memory.json: Persistent conversation memory (auto-created)
- Prompt files: critrules.prompt (required), companion.prompt, lowrules.prompt
- debug.log: Debug logging when --debug flag enabled

APPLICATION ENTRY POINT (main.py):
================================================================

ENHANCED PROMPT MANAGEMENT SYSTEM:
- PromptManager class with intelligent loading and condensation
- Token budget allocation (5,000 tokens for all prompts combined)
- Automatic prompt condensation using LLM when budget exceeded
- Critical validation ensuring critrules.prompt exists
- Graceful handling of missing optional prompt files

MAIN CLASS STRUCTURE:
- DevNameRPGClient: Primary application coordinator with prompt integration
- DebugLogger: Centralized logging system shared across modules
- ApplicationConfig: Hardcoded configuration management (no file creation)
- PromptManager: Handles prompt loading, optimization, and condensation

INITIALIZATION SEQUENCE:
1. Argument parsing and environment setup
2. Module verification and dependency checking
3. Hardcoded configuration loading
4. Debug logger initialization if enabled
5. Async prompt loading and optimization phase
6. CursesInterface creation with prompt injection
7. Signal handlers for graceful shutdown

PROMPT OPTIMIZATION ALGORITHM:
1. Load all available prompt files with graceful missing file handling
2. Calculate combined token usage with conservative estimation
3. Apply LLM-powered condensation if exceeding 5k token budget
4. Individual prompt condensation for files >1/3 of total budget
5. Verification of condensation effectiveness
6. Pass optimized prompts to interface configuration

CONFIGURATION DEFAULTS:
- MCP server: http://127.0.0.1:3456/chat
- MCP model: qwen2.5:14b-instruct-q4_k_m
- MCP timeout: 300 seconds
- Memory max tokens: 16,000
- Interface color theme: classic
- Story pressure decay: 0.05
- Antagonist threshold: 0.6

DEPENDENCY MANAGEMENT:
- Required: curses module (with platform-specific guidance)
- Required: httpx for MCP communication and prompt condensation
- Module verification system for all components
- Graceful degradation messaging for missing dependencies

NCURSES INTERFACE MODULE (nci.py):
================================================================

INTEGRATED PROMPT ARCHITECTURE:
- CursesInterface receives loaded prompts from main.py configuration
- Dynamic system message building with prompt integration
- Story context injection into critrules prompt
- Multi-prompt system: critrules + companion + lowrules
- Real-time prompt status display and management

INTERFACE ARCHITECTURE:
- CursesInterface: Main controller with prompt system integration
- ColorManager: Theme-based color management (classic/dark/bright)
- InputValidator: Token estimation and input validation
- DisplayMessage: Message formatting for display

WINDOW MANAGEMENT:
- Output window: Conversation display with scroll support
- Input window: User input with real-time validation
- Status window: System statistics, connection status, and prompt count
- Automatic dimension calculation with minimum size validation

PROMPT INTEGRATION WORKFLOW:
- _build_system_messages(): Constructs integrated prompt chain
- Primary prompt (critrules) enhanced with story context
- Companion and narrative prompts added as separate system messages
- Custom MCP request with complete message structure
- Fallback to standard send_message if custom approach fails

INPUT PROCESSING:
- Character-by-character input building
- Enter key submission with validation
- Arrow key scrolling through conversation history
- Enhanced commands: /help, /quit, /clear, /stats, /theme, /prompts

MODULE INTERCONNECTS:
- Creates and manages MCPClient, EnhancedMemoryManager, StoryMomentumEngine
- Coordinates message flow between all modules
- Handles async MCP requests while blocking interface input
- Formats story context from SME for prompt integration

COLOR THEME SYSTEM:
- Classic: Cyan user, green assistant, yellow system, red error, blue borders
- Dark: White user, cyan assistant, magenta system, red error, white borders
- Bright: Blue user, green assistant, yellow system, red error, magenta borders

MCP COMMUNICATION MODULE (mcp.py):
================================================================

STREAMLINED ARCHITECTURE:
- MCPClient class with essential HTTP communication
- Single retry mechanism (MCP_MAX_RETRIES = 2)
- Clean context integration from EMM and SME modules
- Eliminated performance monitoring for simplicity

CORE FUNCTIONALITY:
- MCPClient.send_message(): Primary interface with story context integration
- MCPClient._execute_request(): Direct request execution for custom message chains
- MCPClient.test_connection(): Async connection verification
- MCPClient.get_server_info(): Basic diagnostics
- MCPClient.update_system_prompt(): Dynamic prompt modification

ERROR HANDLING:
- Single exception type for all MCP failures
- Debug-only verbose logging
- Graceful fallback when httpx unavailable
- Brief retry delays (1 second) without complex backoff

CONTEXT INTEGRATION DESIGN:
- Story context from SME injected as system message
- Conversation history from EMM limited to last 10 messages
- System prompt + story context + history + user input = complete chain
- Minimal response validation for essential structure checking

ASYNC EXECUTION PATTERN:
- Creates new event loop for each request (avoiding conflicts)
- Simple async/await pattern with httpx.AsyncClient
- Timeout management through httpx configuration
- Clean loop closure after execution

DEFAULT SYSTEM PROMPT:
High-fantasy RPG Game Master instructions for immersive storytelling, rich descriptions, engaging NPCs, and dynamic narrative responses.

ENHANCED MEMORY MANAGER MODULE (emm.py):
================================================================

SEMANTIC ANALYSIS SYSTEM:
- LLM-powered message categorization with 6 semantic categories
- Context-aware analysis using 11-message window (5 before + target + 5 after)
- 3-tier retry system: Full Analysis → Simple Analysis → Binary Decision
- Pure LLM-driven semantic decisions with programmatic fallbacks

SEMANTIC CATEGORIES & PRESERVATION RATIOS:
- story_critical: 0.9 (90% preservation) - Major plot developments, character deaths, world-changing events
- character_focused: 0.8 (80% preservation) - Relationship changes, character development, personality reveals
- relationship_dynamics: 0.8 (80% preservation) - Evolving relationships between characters
- emotional_significance: 0.75 (75% preservation) - Dramatic moments, trust/betrayal, conflict resolution
- world_building: 0.7 (70% preservation) - New locations, lore, cultural info, political changes
- standard: 0.4 (40% preservation) - General interactions, travel, routine activities

PROGRESSIVE CONDENSATION SYSTEM:
- Multi-pass condensation with increasing aggressiveness (up to 3 passes)
- Aggressiveness reduces preservation ratios by 0.1 per pass
- Minimum preservation ratio of 0.2 to prevent critical information loss
- Recent messages (last 5) always protected from condensation

FRAGMENTATION LOGIC:
- LLM identifies multi-category messages and fragments appropriately
- Each fragment assigned individual categories and importance scores
- Multi-category fragments use highest applicable preservation ratio
- Semantic fragmentation for complex messages with multiple elements

MEMORY MANAGEMENT ALGORITHM:
1. Add message → check token threshold → trigger condensation if needed
2. Analyze condensable messages (excluding recent 5) for semantic importance
3. Apply category-weighted preservation ratios adjusted for aggressiveness
4. Group messages for condensation and create LLM-generated summary
5. Replace condensed messages with summary, preserve important messages
6. Repeat up to 3 passes if memory still exceeds threshold

THREAD SAFETY & ASYNC OPERATIONS:
- Thread-safe message storage with locking mechanisms
- Async LLM calls for semantic analysis without blocking interface
- Non-blocking condensation operations in separate event loop
- Safe concurrent access to conversation history

EMM INTERFACE FOR OTHER MODULES:
- get_conversation_for_mcp(): Returns formatted conversation for MCP requests
- add_message(): Thread-safe message storage with automatic condensation
- get_messages(): Retrieves conversation history with optional limits
- get_memory_stats(): Returns current memory utilization statistics
- save_conversation(): Persistent storage with metadata
- load_conversation(): Restore from saved files
- analyze_conversation_patterns(): Debug and analysis functionality

STORY MOMENTUM ENGINE MODULE (sme.py):
================================================================

NARRATIVE PRESSURE SYSTEM:
- Dynamic pressure tracking on 0.0-1.0 scale with pattern-based calculation
- Real-time story arc progression: Setup → Rising Action → Climax → Resolution
- Context-adaptive antagonist generation based on user input patterns
- Rate limiting (2-second cooldown) to prevent spam-induced pressure spikes

PRESSURE CALCULATION ALGORITHM:
- Pattern-based analysis using 6 momentum categories
- Weighted keyword matching with dynamic pressure deltas per category
- Length/complexity factors and punctuation intensity analysis
- Natural pressure decay over time (0.05 per minute) to prevent inflation
- Maximum single increase cap of 0.3 to prevent extreme spikes

MOMENTUM PATTERN CATEGORIES:
- conflict: Keywords trigger +0.15 pressure (fight, attack, defend, battle, combat, strike)
- tension: Keywords trigger +0.12 pressure (danger, threat, fear, worry, concern, risk)
- mystery: Keywords trigger +0.08 pressure (strange, unusual, mysterious, hidden, secret, whisper)
- exploration: Keywords trigger +0.05 pressure (examine, search, look, investigate, explore, discover)
- social: Keywords trigger +0.03 pressure (talk, speak, negotiate, persuade, convince, ask)
- resolution: Keywords trigger -0.10 pressure (resolve, solution, answer, complete, finish, end)

STORY ARC THRESHOLDS:
- Setup Arc: 0.0-0.3 pressure (calm exploration, building tension)
- Rising Action: 0.3-0.7 pressure (escalating conflict)
- Climax: 0.7-0.9 pressure (peak intensity)
- Resolution: 0.9-1.0 pressure (concluding action)

ANTAGONIST MANAGEMENT:
- Context-adaptive antagonist generation at 0.6 pressure threshold
- Dynamic antagonist types based on recent user input analysis:
  * magical_opposition: Corrupted Mage (magic/spell keywords detected)
  * environmental_threat: Ancient Guardian (exploration/dungeon keywords)
  * social_conflict: Corrupt Official (town/people keywords)
  * adaptive_threat: Shadow Entity (default context-adaptive)
- Threat level scaling with current pressure level
- Automatic deactivation during Resolution arc

CONTEXT GENERATION FOR MCP:
- Comprehensive story context including pressure level, arc, narrative state
- Antagonist information when present (name, motivation, threat level, active status)
- Pressure trend analysis (rising/falling/stable) based on recent history
- Narrative state descriptions: calm_exploration, building_tension, escalating_conflict, peak_intensity, concluding_action
- Tension introduction flags and climax approach indicators

SME INTERFACE FOR OTHER MODULES:
- process_user_input(): Updates pressure and returns processing results
- get_story_context(): Returns story context for MCP integration
- reset_story_state(): Clears state for new sessions
- get_pressure_stats(): Returns pressure analytics and statistics
- save_state(): Persistent state management
- load_state(): State restoration
- get_debug_info(): Comprehensive debug information

PROMPT SYSTEM INTEGRATION:
================================================================

PROMPT FILE ARCHITECTURE:
- critrules.prompt: Core GM rules and character control boundaries (REQUIRED)
- companion.prompt: Active companion character definition (OPTIONAL)
- lowrules.prompt: World generation and environmental storytelling rules (OPTIONAL)

COMPANION PROMPT TEMPLATES:
- tsundere_companion.prompt: Seraphina knight companion template
- dandere_companion.prompt: Elena healer companion template
- kuudere_companion.prompt: Lysander mage companion template
- himedere_companion.prompt: Victoria noble companion template

INTELLIGENT PROMPT LOADING:
- Graceful handling of missing files with user warnings
- Automatic token estimation and budget management
- LLM-powered condensation when exceeding 5k token allocation
- Preservation of essential functionality during condensation
- Status display showing active prompt components

PROMPT INTEGRATION WORKFLOW:
1. Load available prompt files during startup
2. Apply condensation if budget exceeded
3. Pass optimized prompts to interface configuration
4. Build dynamic system messages with story context injection
5. Combine prompts + context + history for complete message chain
6. Real-time status display and /prompts command for inspection

TOKEN BUDGET ALLOCATION:
- Total Context Window: 32,000 tokens
- System Prompts: 5,000 tokens (auto-optimized)
- Memory Storage: 16,000 tokens (semantic categorization)
- Input Buffer: Remaining capacity with validation

MODULAR PRESERVATION RULES:
================================================================

1. Breaking interconnects between modules will cause system failure
2. Enhanced Memory Manager maintains specific interface methods for other modules
3. Story Momentum Engine maintains specific interface methods for other modules
4. MCP Communication Module maintains specific interface methods for other modules
5. LLM semantic analysis requires functional MCP configuration
6. Prompt system requires critrules.prompt for core functionality
7. File persistence maintains metadata for debugging and analysis

ERROR HANDLING HIERARCHY:
- main.py: Application-level errors, prompt loading failures, graceful shutdown
- nci.py: Interface errors, display failures, prompt integration errors
- mcp.py: Network errors and communication failures (simplified single exception type)
- emm.py: Memory management errors, LLM analysis failures, condensation errors
- sme.py: Narrative analysis and context generation errors

DEBUGGING INFRASTRUCTURE:
- Centralized DebugLogger class in main.py shared across all modules
- Enhanced memory debugging with semantic analysis tracking
- LLM interaction logging for prompt optimization
- Token usage monitoring and condensation effectiveness metrics
- Story pressure tracking and antagonist lifecycle monitoring
- Prompt loading and condensation process logging
- Simplified MCP communication logging (debug-only verbosity)

PERFORMANCE CONSIDERATIONS:
- LLM semantic analysis adds latency but provides superior memory management
- Async operations prevent interface blocking during condensation
- Context window analysis balances accuracy with computational cost
- Progressive aggressiveness ensures memory constraints are met
- Story pressure calculation optimized for real-time performance
- Rate limiting prevents computational overhead from input spam
- Prompt condensation reduces token usage while preserving functionality
- Simplified MCP retry logic reduces communication overhead

DEPENDENCY REQUIREMENTS:
- Python 3.8+ with curses support
- httpx library for HTTP communication (REQUIRED for full functionality)
- asyncio for non-blocking LLM calls in memory management
- JSON for configuration and history storage
- Threading for thread-safe memory operations

AI MODIFICATION GUIDELINES:
================================================================

1. PRESERVE all programmatic interconnects when modifying any module
2. UPDATE this genai.txt file when making architectural changes
3. MAINTAIN the single responsibility principle for each module
4. DO NOT merge module responsibilities or break the modular boundaries
5. VERIFY that changes do not break the data flow patterns
6. TEST interconnects after any modifications to ensure system integrity
7. LLM prompt modifications require extensive testing for reliability
8. Semantic category changes must update preservation ratios accordingly
9. Story pressure thresholds and antagonist generation logic must remain context-adaptive
10. Pattern-based pressure calculation should maintain balance between categories
11. Prompt system modifications must maintain critrules.prompt as required dependency
12. Token budget changes require corresponding updates to allocation constants
13. Condensation logic changes must preserve essential prompt functionality
14. System message building must maintain prompt integration order and structure

ARCHITECTURAL INNOVATIONS:
================================================================

STORY MOMENTUM ENGINE:
- Mathematical pressure modeling prevents common AI storytelling pitfalls
- Context-adaptive antagonist generation based on player behavior patterns
- Natural pressure decay prevents artificial tension inflation
- Rate limiting prevents exploitation through rapid inputs

INTELLIGENT MEMORY MANAGEMENT:
- LLM-powered semantic categorization with multi-tier retry system
- Progressive condensation with category-weighted preservation
- Thread-safe operations with async LLM calls
- Fragmentation logic for complex multi-category messages

INTEGRATED PROMPT SYSTEM:
- Dynamic prompt loading with intelligent condensation
- Token budget management with automatic optimization
- Story context injection into core GM rules
- Multi-prompt integration with hierarchical message building

TECHNICAL EXCELLENCE:
- Graceful degradation with missing components
- Comprehensive error handling with debug transparency
- Modular architecture with clear separation of concerns
- Real-time status monitoring and user feedback