GENAI.TXT - AI ANALYSIS REFERENCE FOR MODULAR RPG CLIENT
================================================================

PROJECT SCOPE:
Modular RPG client application with ncurses interface and MCP server communication.
Five-module architecture with defined responsibilities and programmatic interconnects.

MODULE ARCHITECTURE:
main.py - Application coordination and lifecycle management
nci.py - Ncurses interface and user interaction
mcp.py - HTTP client for MCP/Ollama server communication  
emm.py - Enhanced Memory Manager with LLM-powered semantic condensation
sme.py - Story Momentum Engine for narrative pressure management

CRITICAL INTERCONNECTS:
main.py → nci.py: Creates CursesInterface instance, manages application lifecycle
nci.py → mcp.py: Sends user messages via MCPClient.send_message()
nci.py → emm.py: Stores/retrieves messages via EnhancedMemoryManager methods
nci.py → sme.py: Updates narrative pressure via StoryMomentumEngine.process_user_input()
mcp.py ← sme.py: Receives story context for enhanced prompting via get_story_context()
mcp.py ← emm.py: Receives conversation history for context via get_conversation_for_mcp()

DATA FLOW:
1. User input captured in nci.py
2. Message stored in emm.py conversation history with automatic semantic analysis
3. Narrative pressure updated in sme.py based on input analysis
4. Context gathered from emm.py and sme.py for mcp.py request
5. MCP request sent to server, response received
6. Response displayed in nci.py interface

DEPENDENCY REQUIREMENTS:
- Python 3.8+ with curses support
- httpx library for HTTP communication (required for LLM semantic analysis)
- asyncio for non-blocking LLM calls in memory management
- JSON for configuration and history storage
- Threading for thread-safe memory operations

CONFIGURATION SYSTEM:
- aurora_config.json: Application settings and MCP server configuration
- chat_history_*.json: Conversation history files with timestamp naming and metadata
- debug_*.log: Debug logging when --debug flag enabled

ENHANCED MEMORY MANAGER SPECIFICS (emm.py):
================================================================

SEMANTIC ANALYSIS SYSTEM:
- LLM-powered message categorization with 6 semantic categories
- Context-aware analysis using 11-message window (5 before + target + 5 after)
- 3-tier retry system: Full Analysis → Simple Analysis → Binary Decision
- No programmatic fallbacks - pure LLM-driven semantic decisions

SEMANTIC CATEGORIES & PRESERVATION RATIOS:
- story_critical: 0.9 (90% preservation) - Major plot developments, character deaths, world-changing events
- character_focused: 0.8 (80% preservation) - Relationship changes, character development, personality reveals
- relationship_dynamics: 0.8 (80% preservation) - Evolving relationships between characters
- emotional_significance: 0.75 (75% preservation) - Dramatic moments, trust/betrayal, conflict resolution
- world_building: 0.7 (70% preservation) - New locations, lore, cultural info, political changes
- standard: 0.4 (40% preservation) - General interactions, travel, routine activities

PROGRESSIVE CONDENSATION SYSTEM:
- Multi-pass condensation with increasing aggressiveness (up to 3 passes)
- Aggressiveness reduces preservation ratios by 0.1 per pass
- Minimum preservation ratio of 0.2 to prevent critical information loss
- Recent messages (last 5) always protected from condensation

FRAGMENTATION LOGIC:
- LLM identifies multi-category messages and fragments them appropriately
- Each fragment assigned individual categories and importance scores
- Multi-category fragments use highest applicable preservation ratio
- Example: "Maria thanked you. Leon sneers from the side" → 2 fragments with different categories

LLM PROMPT DESIGN:
- Structured JSON response formats for reliable parsing
- Context-aware prompts that consider surrounding conversation
- Retry-friendly design with simplified fallback prompts
- Focus on semantic understanding rather than keyword matching

MEMORY MANAGEMENT ALGORITHM:
1. Add message → check token threshold → trigger condensation if needed
2. Analyze condensable messages (excluding recent 5) for semantic importance
3. Apply category-weighted preservation ratios adjusted for aggressiveness level
4. Group messages for condensation and create LLM-generated summary
5. Replace condensed messages with summary, preserve important messages
6. Repeat up to 3 passes if memory still exceeds threshold

THREAD SAFETY & ASYNC OPERATIONS:
- Thread-safe message storage with locking mechanisms
- Async LLM calls for semantic analysis without blocking interface
- Non-blocking condensation operations in separate event loop
- Safe concurrent access to conversation history

STORY MOMENTUM ENGINE SPECIFICS (sme.py):
================================================================

NARRATIVE PRESSURE SYSTEM:
- Dynamic pressure tracking on 0.0-1.0 scale with pattern-based calculation
- Real-time story arc progression: Setup → Rising Action → Climax → Resolution
- Context-adaptive antagonist generation based on user input patterns
- LLM-first approach with single fallback system (no multiple backup antagonists)

PRESSURE CALCULATION ALGORITHM:
- Pattern-based analysis using 6 momentum categories: conflict, tension, mystery, exploration, social, resolution
- Weighted keyword matching with dynamic pressure deltas per category type
- Length/complexity factors and punctuation intensity analysis
- Natural pressure decay over time (0.05 per minute) to prevent artificial inflation
- Rate limiting (2-second cooldown) to prevent spam-induced pressure spikes

MOMENTUM PATTERN CATEGORIES:
- conflict: Keywords trigger +0.15 pressure (combat, battle, attack, fight)
- tension: Keywords trigger +0.12 pressure (danger, threat, fear, worry)
- mystery: Keywords trigger +0.08 pressure (strange, mysterious, hidden, secret)
- exploration: Keywords trigger +0.05 pressure (examine, search, investigate)
- social: Keywords trigger +0.03 pressure (talk, negotiate, persuade)
- resolution: Keywords trigger -0.10 pressure (resolve, solution, complete, finish)

STORY ARC THRESHOLDS:
- Setup Arc: 0.0-0.3 pressure (calm exploration, building tension)
- Rising Action: 0.3-0.7 pressure (escalating conflict)
- Climax: 0.7-0.9 pressure (peak intensity)
- Resolution: 0.9-1.0 pressure (concluding action)

ANTAGONIST MANAGEMENT:
- Single context-adaptive antagonist generation (LLM-first design)
- Introduction threshold: 0.6 pressure level
- Dynamic antagonist types based on recent user input analysis:
  * magical_opposition: Corrupted Mage (magic/spell keywords detected)
  * environmental_threat: Ancient Guardian (exploration/dungeon keywords)
  * social_conflict: Corrupt Official (town/people keywords)
  * adaptive_threat: Shadow Entity (default context-adaptive)
- Threat level scaling with current pressure level
- Automatic deactivation during Resolution arc

CONTEXT GENERATION FOR MCP:
- Comprehensive story context including pressure level, arc, narrative state
- Antagonist information when present (name, motivation, threat level, active status)
- Pressure trend analysis (rising/falling/stable) based on recent history
- Narrative state descriptions: calm_exploration, building_tension, escalating_conflict, peak_intensity, concluding_action
- Tension introduction flags and climax approach indicators

STATE MANAGEMENT:
- Persistent state saving/loading with JSON serialization
- Pressure history tracking (last 200 updates with timestamp pairs)
- User input buffer (last 50 inputs, condensed to 25 when full)
- Thread-safe operations with debug logging integration
- Statistics generation: average/max/min pressure, variance calculation, session duration

PRESSURE TESTING & ANALYSIS:
- Built-in scenario testing: combat, exploration, social, mystery patterns
- Real-time momentum analysis for individual text inputs
- Pattern detection and categorization utilities
- Debug information exposure for system monitoring

MODULAR PRESERVATION RULES:
================================================================

1. Breaking interconnects between modules will cause system failure
2. Enhanced Memory Manager maintains specific interface for other modules:
   - get_conversation_for_mcp(): Returns formatted conversation for MCP requests
   - add_message(): Thread-safe message storage with automatic condensation
   - get_messages(): Retrieves conversation history with optional limits
   - get_memory_stats(): Returns current memory utilization statistics
3. Story Momentum Engine maintains specific interface for other modules:
   - process_user_input(): Updates pressure and returns processing results
   - get_story_context(): Returns story context for MCP integration
   - reset_story_state(): Clears state for new sessions
   - get_pressure_stats(): Returns pressure analytics and statistics
4. LLM semantic analysis requires functional MCP configuration in aurora_config.json
5. File persistence maintains metadata for debugging and analysis

ERROR HANDLING HIERARCHY:
main.py: Application-level errors and graceful shutdown
nci.py: Interface errors and display failures
mcp.py: Network errors and communication failures  
emm.py: Memory management errors, LLM analysis failures, condensation errors
sme.py: Narrative analysis and context generation errors

AI MODIFICATION GUIDELINES:
1. PRESERVE all programmatic interconnects when modifying any module
2. UPDATE this genai.txt file when making architectural changes
3. MAINTAIN the single responsibility principle for each module
4. DO NOT merge module responsibilities or break the modular boundaries
5. VERIFY that changes do not break the data flow patterns
6. TEST interconnects after any modifications to ensure system integrity
7. LLM prompt modifications require extensive testing for reliability
8. Semantic category changes must update preservation ratios accordingly
9. Story pressure thresholds and antagonist generation logic must remain context-adaptive
10. Pattern-based pressure calculation should maintain balance between categories

DEBUGGING INFRASTRUCTURE:
- Centralized DebugLogger class in main.py shared across all modules
- Enhanced memory debugging with semantic analysis tracking
- LLM interaction logging for prompt optimization
- Token usage monitoring and condensation effectiveness metrics
- Conversation pattern analysis for system optimization
- Story pressure tracking and antagonist lifecycle monitoring
- Narrative arc progression analysis and context generation verification

PERFORMANCE CONSIDERATIONS:
- LLM semantic analysis adds latency but provides superior memory management
- Async operations prevent interface blocking during condensation
- Context window analysis balances accuracy with computational cost
- Progressive aggressiveness ensures memory constraints are met
- File persistence optimized with metadata for quick analysis
- Story pressure calculation optimized for real-time performance
- Rate limiting prevents computational overhead from input spam
- Pattern matching uses efficient keyword detection algorithms

SUCCESS METRICS FOR LLM INTEGRATION:
- Target 95%+ success rate for semantic analysis parsing
- Effective token management maintaining conversation coherence
- Robust retry system handling various LLM response formats
- Preservation of narrative continuity through intelligent condensation
- Reliable interconnect functionality with other modules
- Smooth story pressure progression without artificial spikes
- Context-appropriate antagonist generation matching user input patterns
- Effective narrative arc transitions based on pressure thresholds