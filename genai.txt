GENAI.TXT - AI ANALYSIS REFERENCE FOR MODULAR RPG CLIENT
================================================================

PROJECT SCOPE:
Modular RPG client application with ncurses interface and MCP server communication.
Five-module architecture with defined responsibilities and programmatic interconnects.

MODULE ARCHITECTURE:
main.py - Application coordination and lifecycle management
nci.py - Ncurses interface and user interaction
mcp.py - HTTP client for MCP/Ollama server communication (UPDATED - Code Boil optimized)
emm.py - Enhanced Memory Manager with LLM-powered semantic condensation
sme.py - Story Momentum Engine for narrative pressure management

CRITICAL INTERCONNECTS:
main.py → nci.py: Creates CursesInterface instance, manages application lifecycle
nci.py → mcp.py: Sends user messages via MCPClient.send_message()
nci.py → emm.py: Stores/retrieves messages via EnhancedMemoryManager methods
nci.py → sme.py: Updates narrative pressure via StoryMomentumEngine.process_user_input()
mcp.py ← sme.py: Receives story context for enhanced prompting via get_story_context()
mcp.py ← emm.py: Receives conversation history for context via get_conversation_for_mcp()

DATA FLOW:
1. User input captured in nci.py
2. Message stored in emm.py conversation history with automatic semantic analysis
3. Narrative pressure updated in sme.py based on input analysis
4. Context gathered from emm.py and sme.py for mcp.py request
5. MCP request sent to server, response received
6. Response displayed in nci.py interface

DEPENDENCY REQUIREMENTS:
- Python 3.8+ with curses support
- httpx library for HTTP communication (required for LLM semantic analysis)
- asyncio for non-blocking LLM calls in memory management
- JSON for configuration and history storage
- Threading for thread-safe memory operations

CONFIGURATION SYSTEM:
- aurora_config.json: Application settings and MCP server configuration
- chat_history_*.json: Conversation history files with timestamp naming and metadata
- debug_*.log: Debug logging when --debug flag enabled

MCP COMMUNICATION MODULE SPECIFICS (mcp.py) - CODE BOIL OPTIMIZED:
================================================================

SIMPLIFIED ARCHITECTURE:
- Streamlined MCPClient class with essential HTTP communication only
- Single retry mechanism (MCP_MAX_RETRIES = 2) replacing complex fallback systems
- Eliminated performance monitoring and verbose error reporting
- Clean context integration from both EMM and SME modules
- Brand-neutral naming throughout (DevName RPG Client)

CORE FUNCTIONALITY:
- MCPClient.send_message(): Primary interface for message transmission
  * Integrates story_context from SME module
  * Accepts conversation_history from EMM module
  * Handles up to 10 historical messages for efficiency
  * Returns clean response content or raises exception
- MCPClient.test_connection(): Simple async connection verification
- MCPClient.get_server_info(): Basic diagnostics without performance metrics
- MCPClient.update_system_prompt(): Dynamic prompt modification capability

SIMPLIFIED ERROR HANDLING:
- Single exception type for all MCP failures
- Debug-only verbose logging (no user-facing error verbosity)
- Graceful fallback when httpx unavailable
- Brief retry delays (1 second) without complex backoff algorithms

CONTEXT INTEGRATION DESIGN:
- Story context from SME injected as system message: "Story Context: {context}"
- Conversation history from EMM limited to last 10 messages for token efficiency
- System prompt + story context + history + user input = complete message chain
- No redundant validation beyond essential response structure checking

CONFIGURATION CONSTANTS:
- MCP_SERVER_URL: "http://localhost:11434/api/chat"
- MCP_MODEL: "qwen2.5:14b-instruct"
- MCP_TIMEOUT: 30 seconds
- MCP_MAX_RETRIES: 2 attempts
- DEFAULT_SYSTEM_PROMPT: High-fantasy RPG Game Master instructions

ASYNC EXECUTION PATTERN:
- Creates new event loop for each request (avoiding loop conflicts)
- Simple async/await pattern with httpx.AsyncClient
- Timeout management through httpx configuration
- Clean loop closure after execution

UTILITY FUNCTIONS:
- validate_mcp_request(): Payload structure validation
- quick_mcp_test(): Standalone connection testing without client instantiation
- Module test suite for standalone verification

REMOVED COMPLEXITY (Code Boil Objectives):
- Performance monitoring and statistics collection
- Multiple retry strategies and complex backoff algorithms
- Verbose user-facing error messages and marketing language
- Request/response validation beyond essential structure
- Aurora branding replaced with DevName RPG Client
- Extensive debugging infrastructure (simplified to essential logging)

ENHANCED MEMORY MANAGER SPECIFICS (emm.py):
================================================================

SEMANTIC ANALYSIS SYSTEM:
- LLM-powered message categorization with 6 semantic categories
- Context-aware analysis using 11-message window (5 before + target + 5 after)
- 3-tier retry system: Full Analysis → Simple Analysis → Binary Decision
- No programmatic fallbacks - pure LLM-driven semantic decisions

SEMANTIC CATEGORIES & PRESERVATION RATIOS:
- story_critical: 0.9 (90% preservation) - Major plot developments, character deaths, world-changing events
- character_focused: 0.8 (80% preservation) - Relationship changes, character development, personality reveals
- relationship_dynamics: 0.8 (80% preservation) - Evolving relationships between characters
- emotional_significance: 0.75 (75% preservation) - Dramatic moments, trust/betrayal, conflict resolution
- world_building: 0.7 (70% preservation) - New locations, lore, cultural info, political changes
- standard: 0.4 (40% preservation) - General interactions, travel, routine activities

PROGRESSIVE CONDENSATION SYSTEM:
- Multi-pass condensation with increasing aggressiveness (up to 3 passes)
- Aggressiveness reduces preservation ratios by 0.1 per pass
- Minimum preservation ratio of 0.2 to prevent critical information loss
- Recent messages (last 5) always protected from condensation

FRAGMENTATION LOGIC:
- LLM identifies multi-category messages and fragments them appropriately
- Each fragment assigned individual categories and importance scores
- Multi-category fragments use highest applicable preservation ratio
- Example: "Maria thanked you. Leon sneers from the side" → 2 fragments with different categories

LLM PROMPT DESIGN:
- Structured JSON response formats for reliable parsing
- Context-aware prompts that consider surrounding conversation
- Retry-friendly design with simplified fallback prompts
- Focus on semantic understanding rather than keyword matching

MEMORY MANAGEMENT ALGORITHM:
1. Add message → check token threshold → trigger condensation if needed
2. Analyze condensable messages (excluding recent 5) for semantic importance
3. Apply category-weighted preservation ratios adjusted for aggressiveness level
4. Group messages for condensation and create LLM-generated summary
5. Replace condensed messages with summary, preserve important messages
6. Repeat up to 3 passes if memory still exceeds threshold

THREAD SAFETY & ASYNC OPERATIONS:
- Thread-safe message storage with locking mechanisms
- Async LLM calls for semantic analysis without blocking interface
- Non-blocking condensation operations in separate event loop
- Safe concurrent access to conversation history

EMM INTERFACE FOR OTHER MODULES:
- get_conversation_for_mcp(): Returns formatted conversation for MCP requests
- add_message(): Thread-safe message storage with automatic condensation
- get_messages(): Retrieves conversation history with optional limits
- get_memory_stats(): Returns current memory utilization statistics

STORY MOMENTUM ENGINE SPECIFICS (sme.py):
================================================================

NARRATIVE PRESSURE SYSTEM:
- Dynamic pressure tracking on 0.0-1.0 scale with pattern-based calculation
- Real-time story arc progression: Setup → Rising Action → Climax → Resolution
- Context-adaptive antagonist generation based on user input patterns
- LLM-first approach with single fallback system (no multiple backup antagonists)

PRESSURE CALCULATION ALGORITHM:
- Pattern-based analysis using 6 momentum categories: conflict, tension, mystery, exploration, social, resolution
- Weighted keyword matching with dynamic pressure deltas per category type
- Length/complexity factors and punctuation intensity analysis
- Natural pressure decay over time (0.05 per minute) to prevent artificial inflation
- Rate limiting (2-second cooldown) to prevent spam-induced pressure spikes

MOMENTUM PATTERN CATEGORIES:
- conflict: Keywords trigger +0.15 pressure (combat, battle, attack, fight)
- tension: Keywords trigger +0.12 pressure (danger, threat, fear, worry)
- mystery: Keywords trigger +0.08 pressure (strange, mysterious, hidden, secret)
- exploration: Keywords trigger +0.05 pressure (examine, search, investigate)
- social: Keywords trigger +0.03 pressure (talk, negotiate, persuade)
- resolution: Keywords trigger -0.10 pressure (resolve, solution, complete, finish)

STORY ARC THRESHOLDS:
- Setup Arc: 0.0-0.3 pressure (calm exploration, building tension)
- Rising Action: 0.3-0.7 pressure (escalating conflict)
- Climax: 0.7-0.9 pressure (peak intensity)
- Resolution: 0.9-1.0 pressure (concluding action)

ANTAGONIST MANAGEMENT:
- Single context-adaptive antagonist generation (LLM-first design)
- Introduction threshold: 0.6 pressure level
- Dynamic antagonist types based on recent user input analysis:
  * magical_opposition: Corrupted Mage (magic/spell keywords detected)
  * environmental_threat: Ancient Guardian (exploration/dungeon keywords)
  * social_conflict: Corrupt Official (town/people keywords)
  * adaptive_threat: Shadow Entity (default context-adaptive)
- Threat level scaling with current pressure level
- Automatic deactivation during Resolution arc

CONTEXT GENERATION FOR MCP:
- Comprehensive story context including pressure level, arc, narrative state
- Antagonist information when present (name, motivation, threat level, active status)
- Pressure trend analysis (rising/falling/stable) based on recent history
- Narrative state descriptions: calm_exploration, building_tension, escalating_conflict, peak_intensity, concluding_action
- Tension introduction flags and climax approach indicators

SME INTERFACE FOR OTHER MODULES:
- process_user_input(): Updates pressure and returns processing results
- get_story_context(): Returns story context for MCP integration
- reset_story_state(): Clears state for new sessions
- get_pressure_stats(): Returns pressure analytics and statistics

STATE MANAGEMENT:
- Persistent state saving/loading with JSON serialization
- Pressure history tracking (last 200 updates with timestamp pairs)
- User input buffer (last 50 inputs, condensed to 25 when full)
- Thread-safe operations with debug logging integration
- Statistics generation: average/max/min pressure, variance calculation, session duration

MODULAR PRESERVATION RULES:
================================================================

1. Breaking interconnects between modules will cause system failure
2. Enhanced Memory Manager maintains specific interface for other modules:
   - get_conversation_for_mcp(): Returns formatted conversation for MCP requests
   - add_message(): Thread-safe message storage with automatic condensation
   - get_messages(): Retrieves conversation history with optional limits
   - get_memory_stats(): Returns current memory utilization statistics
3. Story Momentum Engine maintains specific interface for other modules:
   - process_user_input(): Updates pressure and returns processing results
   - get_story_context(): Returns story context for MCP integration
   - reset_story_state(): Clears state for new sessions
   - get_pressure_stats(): Returns pressure analytics and statistics
4. MCP Communication Module maintains specific interface for other modules:
   - send_message(): Primary communication method with context integration
   - test_connection(): Connection verification for diagnostics
   - get_server_info(): Basic server information without performance metrics
   - update_system_prompt(): Dynamic prompt modification capability
5. LLM semantic analysis requires functional MCP configuration in aurora_config.json
6. File persistence maintains metadata for debugging and analysis

ERROR HANDLING HIERARCHY:
main.py: Application-level errors and graceful shutdown
nci.py: Interface errors and display failures
mcp.py: Network errors and communication failures (SIMPLIFIED - single exception type)
emm.py: Memory management errors, LLM analysis failures, condensation errors
sme.py: Narrative analysis and context generation errors

CODE BOIL COMPLIANCE:
================================================================

APPLIED OPTIMIZATIONS (mcp.py):
- ✅ External documentation reference (genai.txt) replaces verbose comment blocks
- ✅ Marketing language removed (Aurora → DevName RPG Client)
- ✅ Single retry mechanism replaces complex fallback systems
- ✅ Debug-only error reporting (no user-facing verbosity)
- ✅ Performance monitoring eliminated
- ✅ LLM-first design with clean context integration
- ✅ Preserved all programmatic interconnects

REMAINING CODE BOIL TARGETS:
- emm.py: Enhanced Memory Manager optimization (Phase 1 - Highest Priority)
- sme.py: Story Momentum Engine optimization (Phase 1 - High Priority)
- nci.py: Ncurses Interface cleanup (Phase 3 - Medium Priority)
- main.py: Application coordination cleanup (Phase 3 - Lowest Priority)

AI MODIFICATION GUIDELINES:
================================================================

1. PRESERVE all programmatic interconnects when modifying any module
2. UPDATE this genai.txt file when making architectural changes
3. MAINTAIN the single responsibility principle for each module
4. DO NOT merge module responsibilities or break the modular boundaries
5. VERIFY that changes do not break the data flow patterns
6. TEST interconnects after any modifications to ensure system integrity
7. LLM prompt modifications require extensive testing for reliability
8. Semantic category changes must update preservation ratios accordingly
9. Story pressure thresholds and antagonist generation logic must remain context-adaptive
10. Pattern-based pressure calculation should maintain balance between categories
11. APPLY Code Boil principles: minimize verbosity, eliminate marketing language, simplify error handling
12. MAINTAIN essential functionality while reducing boilerplate and redundancy

DEBUGGING INFRASTRUCTURE:
- Centralized DebugLogger class in main.py shared across all modules
- Enhanced memory debugging with semantic analysis tracking
- LLM interaction logging for prompt optimization
- Token usage monitoring and condensation effectiveness metrics
- Conversation pattern analysis for system optimization
- Story pressure tracking and antagonist lifecycle monitoring
- Narrative arc progression analysis and context generation verification
- Simplified MCP communication logging (debug-only verbosity)

PERFORMANCE CONSIDERATIONS:
- LLM semantic analysis adds latency but provides superior memory management
- Async operations prevent interface blocking during condensation
- Context window analysis balances accuracy with computational cost
- Progressive aggressiveness ensures memory constraints are met
- File persistence optimized with metadata for quick analysis
- Story pressure calculation optimized for real-time performance
- Rate limiting prevents computational overhead from input spam
- Pattern matching uses efficient keyword detection algorithms
- Simplified MCP retry logic reduces communication overhead

SUCCESS METRICS FOR LLM INTEGRATION:
- Target 95%+ success rate for semantic analysis parsing
- Effective token management maintaining conversation coherence
- Robust retry system handling various LLM response formats
- Preservation of narrative continuity through intelligent condensation
- Reliable interconnect functionality with other modules
- Smooth story pressure progression without artificial spikes
- Context-appropriate antagonist generation matching user input patterns
- Effective narrative arc transitions based on pressure thresholds
- Streamlined MCP communication without performance degradation

PRESSURE TESTING & ANALYSIS:
- Built-in scenario testing: combat, exploration, social, mystery patterns
- Real-time momentum analysis for individual text inputs
- Pattern detection and categorization utilities
- Debug information exposure for system monitoring
- MCP connection testing and server verification
- Simplified diagnostic reporting for troubleshooting

ARCHITECTURE EVOLUTION TRACKING:
================================================================

COMPLETED OPTIMIZATIONS:
- mcp.py: Full Code Boil optimization completed
  * Removed verbose comment blocks → genai.txt reference
  * Eliminated marketing language and Aurora branding
  * Simplified retry logic from complex fallback to single mechanism
  * Removed performance monitoring infrastructure
  * Streamlined error handling to debug-only verbosity
  * Maintained all critical module interconnects

NEXT PHASE TARGETS:
- emm.py: Memory manager optimization (remove verbose logging, simplify condensation reporting)
- sme.py: Story momentum optimization (consolidate antagonist fallbacks, eliminate static examples)
- Cross-module pattern identification and consolidation
- Final integration testing and verification
