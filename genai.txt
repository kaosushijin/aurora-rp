# GENAI.TXT - AI ANALYSIS REFERENCE FOR DEVNAME RPG CLIENT
================================================================

## PROJECT SCOPE
Terminal-based RPG storytelling client leveraging Large Language Model capabilities through MCP (Model Context Protocol). Features hub-and-spoke orchestration architecture with complete LLM semantic analysis, dynamic coordinate system, and background processing for responsive user experience.

## PROGRAM FLOW ANALYSIS (Commit: cf19344aba86381de89d4c34e1f5ca4cf20c1f8c - FULLY OPERATIONAL)

### Application Entry Point (main.py)
1. **Environment Setup**: Module verification, dependency checks, terminal capability validation
2. **Prompt Management**: Load and validate prompt files (critrules.prompt required, companion.prompt and lowrules.prompt optional)
3. **Configuration**: Hardcoded configuration values with MCP server on localhost:3456
4. **Application Initialization**: Create DevNameRPGClient with orchestrator pattern
5. **Interface Launch**: Transfer control to NCursesUIController via Orchestrator for main program execution

### Core Execution Flow
```
main() → DevNameRPGClient.run() → Orchestrator.run() → NCursesUIController.run() → UI main loop
```

## CURRENT MODULE ARCHITECTURE - HUB-AND-SPOKE PATTERN

### Hub Module (Central Coordinator)

**orch.py** - Central orchestrator, hub of hub-and-spoke architecture
- Manages all module communication and coordination
- Only module with direct access to mcp.py (exclusive LLM communication channel)
- Handles UI callbacks and complete input processing pipeline
- Coordinates background analysis threads and semantic processing
- **Key Features**: 
  - Complete user input → semantic validation → LLM → response pipeline
  - Graceful error handling with debug logging
  - Background analysis coordination (every 15 messages)
  - Thread management for non-blocking operations

### Primary Spoke Modules

**main.py** - Application coordination, prompt management, lifecycle control
- `PromptManager`: Automatic prompt loading with token estimation
- `DevNameRPGClient`: Main application coordinator with signal handling
- `ApplicationConfig`: Hardcoded configuration (MCP server: localhost:3456, model: qwen2.5:14b-instruct-q4_k_m)
- **Token Budget**: 5,000 tokens for system prompts

**ncui.py** - NCurses UI Controller, pure display and input management
- Terminal management, window creation, input handling, message display
- Communicates only through orchestrator callback
- Multi-line input with cursor navigation and word wrapping
- Color themes (classic/dark/bright) with dynamic switching
- Scrolling support with PgUp/PgDn navigation
- Command system (/help, /clear, /stats, /theme, /quit, /analyze)

**uilib.py** - Consolidated UI library, all UI components
- `calculate_box_layout()`: Dynamic window dimension calculations
- `TerminalManager`: Terminal size management with resize handling
- `ColorManager`: Theme management with 3 built-in schemes
- `ScrollManager`: Navigation state with bounds protection
- `MultiLineInput`: Advanced text input with word wrapping
- `DisplayMessage`: Message formatting and type differentiation

### Supporting Spoke Modules

**emm.py** - Enhanced Memory Manager
- Message storage and retrieval with auto-save to memory.json
- Thread-safe operations with RLock protection
- Background auto-save to prevent UI blocking
- **Memory Management**: 25,000 token limit with semantic condensation requests
- **SME Integration**: Stores and retrieves momentum state
- **Message Types**: USER, ASSISTANT, SYSTEM, MOMENTUM_STATE

**sme.py** - Story Momentum Engine
- Narrative progression tracking with pattern-based analysis
- Story arc phases: SETUP, RISING, CLIMAX, RESOLUTION
- Antagonist management with threat level tracking
- Pressure level system (0.0-10.0 scale) with floor ratcheting
- Narrative time tracking for story progression
- State persistence through EMM integration

**sem.py** - Semantic Analysis Engine
- Input validation for orchestrator pipeline
- Semantic categorization with 6-tier importance system:
  - story_critical (90% preservation)
  - character_focused (80% preservation)
  - relationship_dynamics (70% preservation)
  - emotional_significance (60% preservation)
  - world_building (50% preservation)
  - standard (40% preservation)
- 5-strategy JSON parsing for LLM response reliability
- Content condensation prompting for memory management

**mcp.py** - MCP Client for LLM communication
- Model Context Protocol for qwen2.5:14b-instruct-q4_k_m
- Server URL: http://127.0.0.1:3456/chat
- Exclusive access through orchestrator (no direct spoke access)
- 5-minute timeout for ollama processing
- Fallback operation without httpx dependency

## CRITICAL INTERCONNECTS - HUB-AND-SPOKE ENFORCEMENT

### Communication Rules
1. **All modules communicate ONLY through orch.py** - no direct module-to-module communication
2. **MCP access is exclusive to orchestrator** - spokes cannot call LLM directly
3. **UI uses orchestrator callback** for all operations beyond display
4. **Semantic operations go through orchestrator** to coordinate with sem.py

### Data Flow Pipeline
```
User Input → ncui.py → orch.py → sem.py (validation) → emm.py (storage) → mcp.py (LLM) → emm.py (response storage) → ncui.py (display)
```

### Module Initialization Order (in orch.py)
1. EnhancedMemoryManager (storage, no dependencies)
2. SemanticAnalysisEngine (analysis, no dependencies)
3. StoryMomentumEngine (state tracking with threading.Lock)
4. MCPClient (LLM communication, configured with prompts)
5. NCursesUIController (UI with orchestrator callback)
6. Background services startup

## TECHNICAL SPECIFICATIONS

### Dependencies
- **Python 3.8+** with curses support
- **httpx** (optional but recommended for MCP communication)
- **asyncio** for asynchronous operations
- **threading** for background processing
- **json** for state persistence

### Configuration
- **Context Window**: 32,000 tokens theoretical limit
- **Memory Budget**: 25,000 tokens (conservative)
- **System Prompt Budget**: 5,000 tokens
- **Max User Input**: 2,000 tokens
- **Minimum Terminal**: 80x24 characters
- **Analysis Interval**: Every 15 messages

### File Structure
```
Required Files (all in root directory):
- main.py          # Application entry point
- orch.py          # Central orchestrator hub
- ncui.py          # UI controller
- uilib.py         # UI component library
- emm.py           # Memory manager
- sme.py           # Story momentum engine
- sem.py           # Semantic analysis engine
- mcp.py           # MCP client
- critrules.prompt # Required system prompt

Optional Files:
- companion.prompt # Character definitions
- lowrules.prompt  # Narrative guidelines
- memory.json      # Conversation history
- debug.log        # Debug output
```

## CRITICAL PRESERVATION REQUIREMENTS

### Architecture Integrity
1. **Hub-and-spoke pattern MUST be preserved** - all communication through orchestrator
2. **No direct module-to-module communication** except through orchestrator callbacks
3. **MCP access remains exclusive to orchestrator** - no spoke module should import mcp.py
4. **Thread safety in all shared operations** - especially EMM and SME
5. **Background operations must remain non-blocking** - UI responsiveness is critical
6. **5-strategy JSON parsing** must be preserved for LLM response reliability
7. **Semantic categorization hierarchy** must be maintained for content preservation
8. **15-message analysis cycle timing** should be preserved for momentum tracking

### Development Guidelines
- **Always update through orchestrator** when adding cross-module features
- **Maintain thread safety** with proper locking in shared resources
- **Test background operations** after any threading changes
- **Preserve UI responsiveness** - never block the main UI thread
- **Update genai.txt** when making architectural changes
- **Follow hub-and-spoke pattern** - resist temptation for direct module communication
- **Use orchestrator callbacks** for any module that needs external data

### Known Issues and Improvements

**Critical Issues:**
- **Token Budget Coordination**: EMM and MCP don't coordinate token usage, risking context overflow
- **Display Buffer Accumulation**: UI display buffer needs periodic cleanup to prevent memory growth
- **Error Recovery**: Some error paths don't gracefully recover (e.g., MCP timeout handling)

**Planned Improvements:**
1. **Token Management System**: Implement token counting during MCP request building
2. **Display Buffer Management**: Add periodic cleanup of old display messages
3. **Enhanced Error Recovery**: Improve fallback behavior for network/LLM failures
4. **Momentum Analysis Optimization**: Consider caching recent analysis results
5. **Semantic Category Tuning**: Adjust preservation ratios based on story genre
6. **Command Extensions**: Add more user commands for story control

## INSTRUCTIONS FOR CLAUDE SONNET 4

When rewriting this document for your own understanding:

1. **Preserve the hub-and-spoke architecture explanation** - This is the most critical design pattern. The orchestrator (orch.py) is the only module that coordinates between other modules. Breaking this pattern will cause integration failures.

2. **Maintain the module dependency order** - The initialization sequence in orch.py is critical. Modules must be initialized in the correct order to avoid circular dependencies.

3. **Keep the token budget warnings** - The lack of coordination between EMM memory usage and MCP request building is a real issue that needs addressing.

4. **Document any new interconnects explicitly** - If you add any module communication, it MUST go through the orchestrator. Document the callback pattern used.

5. **Update the data flow pipeline** - If you modify how data moves through the system, update the pipeline diagram to reflect the new flow.

6. **Note threading considerations** - Background operations are critical for UI responsiveness. Any new background operations should be documented with their threading model.

7. **Preserve the semantic categories** - The 6-tier importance system is calibrated for RPG storytelling. Changes should be justified by testing.

8. **Keep the file structure flat** - All modules in root directory. This simplifies imports and makes the architecture clear.

## USAGE INSTRUCTIONS

### Basic Operation
```bash
# Run with default configuration
python main.py

# Run with debug logging
python main.py --debug

# Required: critrules.prompt must exist in current directory
# Optional: companion.prompt and lowrules.prompt for additional context
```

### Terminal Requirements
- Minimum size: 80x24 characters
- Unicode support recommended
- Color terminal for best experience

### Commands
- `/help` - Show available commands
- `/clear` - Clear conversation history
- `/stats` - Display memory statistics
- `/theme <n>` - Switch color theme (1=classic, 2=dark, 3=bright)
- `/analyze` - Force momentum analysis
- `/quit` - Exit application

### Debug Features
- Comprehensive logging to debug.log
- Thread operation visibility
- Semantic categorization decisions
- LLM request/response logging
- Memory management operations

---
Last Updated: Analysis of commit cf19344aba86381de89d4c34e1f5ca4cf20c1f8c
Project State: Hub-and-spoke architecture with complete LLM integration and responsive interface
Architecture Pattern: Orchestrator-based coordination with strict module isolation