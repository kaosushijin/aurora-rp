# GENAI.TXT - AI ANALYSIS REFERENCE FOR DEVNAME RPG CLIENT
================================================================

## PROJECT OVERVIEW
Terminal-based RPG storytelling client leveraging Large Language Models through MCP (Model Context Protocol). Features hub-and-spoke orchestration architecture, comprehensive semantic analysis pipeline, and responsive multi-threaded processing with stateless UI design.

## CRITICAL ARCHITECTURE: HUB-AND-SPOKE PATTERN

The system implements strict hub-and-spoke architecture where:
- **ALL inter-module communication flows through orch.py**
- **NO direct module-to-module calls allowed**
- **MCP access is orchestrator-exclusive**
- **UI maintains zero message state**

## PROGRAM EXECUTION FLOW

### Startup Sequence
```
main.py → Orchestrator.__init__() → Module initialization → UI.run() → Event loop
```

1. **Module Verification**: Validate all `.py` files present in root
2. **Prompt Loading**: Load and concatenate prompt files (5000 token budget)
3. **Configuration**: Hardcoded MCP server at localhost:3456
4. **Orchestrator Creation**: Initialize hub with all service modules
5. **UI Launch**: Transfer control to NCursesUIController

### Module Initialization Order (Critical)
1. **EnhancedMemoryManager** - Storage layer, no dependencies
2. **SemanticAnalysisEngine** - Analysis engine, no dependencies  
3. **StoryMomentumEngine** - State tracking with threading.Lock
4. **MCPClient** - LLM interface, orchestrator-exclusive
5. **NCursesUIController** - Stateless display, orchestrator callback
6. **Background Services** - Auto-save and analysis threads

## MODULE ARCHITECTURE

### Hub Module: orch.py (Orchestrator)

**Core Responsibilities:**
- Central coordination for all module communication
- Exclusive MCP client access for LLM requests
- Background thread management (analysis, auto-save)
- State synchronization across modules
- Input validation pipeline coordination

**Key Methods:**
- `_handle_ui_callback()`: Process all UI requests
- `_process_user_input()`: Complete input→storage→LLM pipeline
- `_make_llm_request()`: ONLY function calling MCP
- `_trigger_periodic_analysis()`: 15-message momentum analysis
- `_get_message_history()`: Provide messages to stateless UI

**Critical Implementation:**
- User message stored IMMEDIATELY before LLM processing
- Background LLM thread allows instant UI echo
- Force auto-save after message storage for persistence

### Spoke Modules

#### main.py - Application Lifecycle
- `PromptManager`: Token-aware prompt concatenation
- `DevNameRPGClient`: Signal handling and orchestrator launch
- `ApplicationConfig`: Hardcoded configuration values
- Module presence verification
- Debug logger initialization

#### ncui.py - Stateless UI Controller
**CRITICAL: Zero message storage - all content from orchestrator**

**Key Features:**
- Dynamic terminal layout with 90/10 split
- Multi-line input with viewport scrolling
- PgUp/PgDn scrollback navigation
- Color themes: classic, dark, bright, nord, solarized, monokai
- Command system: /help, /stats, /analyze, /theme, /clear, /quit

**Stateless Operations:**
- `_process_display_updates()`: Fetch messages every cycle
- `_handle_user_input()`: Pass to orchestrator immediately
- No display_buffer or message_ids tracking
- Fresh data retrieval on every refresh

**Known Issues:**
- Home/End keys not working (codes 262/360 detected but not handled)
- Ctrl+Left/Right not working (codes 554/569 detected but not handled)

#### uilib.py - Consolidated UI Components

**Layout System:**
- `calculate_box_layout()`: Dynamic geometry with status line fix
- `TerminalManager`: Resize detection and validation
- MIN_SCREEN_WIDTH=80, MIN_SCREEN_HEIGHT=24

**Multi-line Input:**
- Viewport-based scrolling for long input
- Word-boundary wrapping with space preservation
- Cascading content flow on deletion (Phase 2 implementation)
- Arrow key navigation with scroll boundaries
- Double-enter or punctuation submission logic

**Fixed Issues:**
- Layout overflow preventing status line visibility
- Premature word wrapping at max_width-5
- Cascading deletion flow through multiple lines

**Remaining Issues:**
- Home/End key navigation to buffer start/end
- Ctrl+arrow word jumping inconsistent

#### emm.py - Enhanced Memory Manager

**Core Features:**
- Thread-safe storage with RLock protection
- Atomic file operations (temp→rename pattern)
- 25,000 token budget with overflow protection
- MessageType enum: USER, ASSISTANT, SYSTEM, MOMENTUM_STATE
- Background auto-save thread (1-second intervals)

**Integration Points:**
- Stores semantic categories from sem.py
- Maintains momentum state for sme.py
- Provides conversation context to MCP
- Handles condensation requests from orchestrator

**Message Structure:**
```python
Message:
  - id: UUID
  - content: str
  - message_type: MessageType
  - timestamp: ISO format
  - token_estimate: chars/4
  - content_category: semantic category
  - condensed: bool
```

#### sem.py - Semantic Analysis Engine

**Input Validation (`validate_input()`):**
- Length check (≤4000 chars)
- Category detection: command, narrative, meta, query
- Confidence scoring (0.0-1.0)
- Token estimation

**Semantic Categorization:**
```
story_critical:        90% preservation
character_focused:     80% preservation
relationship_dynamics: 70% preservation
emotional_significance: 60% preservation
world_building:        50% preservation
standard:             40% preservation
```

**LLM Analysis Pipeline:**
1. Build semantic prompt
2. Request through orchestrator callback
3. 5-strategy response parsing
4. Pattern-based fallback
5. Category injection

**CRITICAL FIX**: Added missing `validate_input()` method blocking input pipeline

#### sme.py - Story Momentum Engine

**Simplified Implementation:**
- Pattern-based momentum detection (no direct LLM)
- Narrative time tracking with duration patterns
- Pressure system (0.0-1.0) with floor ratcheting
- Story arc progression: SETUP→RISING→CLIMAX→RESOLUTION
- Basic antagonist threshold detection

**Pattern Detection:**
- exploration_patterns
- tension_patterns (increase pressure)
- conflict_patterns (increase pressure)
- resolution_patterns (decrease pressure)

**Narrative Time:**
- Activity-based duration estimates
- Cumulative narrative seconds tracking
- Exchange counting for analysis triggers

#### mcp.py - MCP Client

**Configuration:**
- Server: http://localhost:3456/chat
- Model: qwen3:8b-q4_K_M
- Timeout: 300 seconds (5 minutes)
- Exclusive orchestrator access

**Key Features:**
- HTTP-based ollama MCP server communication
- System prompt concatenation from multiple files
- 32,000 token context window
- Graceful httpx fallback
- Think block removal from responses

## DATA FLOW PIPELINES

### User Input Pipeline
```
User Input → ncui.py → Orchestrator → sem.validate_input()
                ↓
            emm.add_message() [IMMEDIATE]
                ↓
            Background LLM Thread → mcp.send_message()
                ↓
            emm.add_message() [Assistant response]
                ↓
            ncui.py display update
```

### Analysis Pipeline (Every 15 messages)
```
Orchestrator timer → sem.analyze_conversation()
                ↓
            sme.analyze_momentum()
                ↓
            Update message categories
                ↓
            Check condensation threshold
```

## TOKEN BUDGET MANAGEMENT

### Current Allocations
- **Context Window**: 32,000 tokens total
- **System Prompts**: 5,000 tokens (auto-condensed)
- **Memory Budget**: 25,000 tokens (triggers condensation)
- **User Input Max**: 2,000 tokens (500 chars typical)
- **LLM Response Reserve**: 5,000 tokens

### Condensation Strategy
1. Check token count > 25,000
2. Get condensation candidates (exclude recent 5)
3. Group by semantic category
4. Preserve high-importance verbatim
5. Summarize low-importance groups
6. Replace with condensed messages

## CRITICAL FIXES IMPLEMENTED

1. **User Echo Fix**: Store user message BEFORE LLM processing
2. **Semantic Validation**: Added missing `validate_input()` method
3. **Layout Overflow**: Fixed status line positioning calculation
4. **Word Wrap**: Removed premature max_width-5 trigger
5. **Cascading Flow**: Implemented proper multi-line content reflow
6. **Debug Logger**: Fixed method call pattern with null safety
7. **MCP Prompts**: Concatenate all prompts with /no_think prefix

## KNOWN ISSUES

### Multi-line Input
- Home/End keys detected (262/360) but not navigating
- Ctrl+Left/Right detected (554/569) but not working
- Contrast with working PgUp/PgDn implementation

### Token Coordination
- EMM and MCP maintain separate budgets
- No centralized token manager
- Risk of context overflow

### Antagonist System
- Simplified pattern-based detection only
- Full LLM generation disabled
- Basic threshold triggers at pressure > 0.5

## TESTING CONSIDERATIONS

### Critical Test Points
1. **Immediate Echo**: User message appears before LLM response
2. **Semantic Categories**: Verify preservation ratios
3. **Condensation**: Maintain narrative continuity
4. **Momentum Tracking**: Pressure increases with conflict
5. **Antagonist Generation**: Appears at pressure > 0.5
6. **Thread Safety**: No deadlocks or race conditions

### Debug Monitoring
Enable with `--debug` flag:
- Watch "SEM:" entries for categorization
- Watch "SME:" entries for momentum
- Watch "EMM:" entries for storage
- Watch "MCP:" entries for LLM calls
- Watch "ORCHESTRATOR:" for coordination

## DEVELOPMENT GUIDELINES

### Architecture Rules
1. **NO direct module communication** - use orchestrator
2. **NO MCP access from spokes** - orchestrator exclusive
3. **NO message storage in UI** - keep stateless
4. **NO blocking operations** - use threads
5. **ALWAYS update genai.txt** for architectural changes

### File Structure
```
Required (root directory):
├── main.py             # Entry point
├── orch.py             # Orchestrator hub
├── ncui.py             # Stateless UI
├── uilib.py            # UI components
├── emm.py              # Memory manager
├── sem.py              # Semantic analysis
├── sme.py              # Momentum engine
├── mcp.py              # LLM client
└── critrules.prompt    # Core rules (required)

Optional:
├── companion.prompt    # Character definitions
├── lowrules.prompt     # Narrative guidelines
├── memory.json         # Conversation persistence
└── debug.log           # Debug output
```

---
*Last Updated: December 2024*
*Architecture: Hub-and-spoke with stateless UI*
*Status: Production-ready with known input key issues*