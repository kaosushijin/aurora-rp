GENAI.TXT - AI ANALYSIS REFERENCE FOR MODULAR RPG CLIENT
================================================================

PROJECT SCOPE:
Modular RPG client application with ncurses interface and MCP server communication.
Five-module architecture with defined responsibilities and programmatic interconnects.

MODULE ARCHITECTURE:
main.py - Application coordination and lifecycle management
nci.py - Ncurses interface and user interaction
mcp.py - HTTP client for MCP/Ollama server communication
emm.py - Enhanced Memory Manager with LLM-powered semantic condensation
sme.py - Story Momentum Engine for narrative pressure management

CRITICAL INTERCONNECTS:
main.py → nci.py: Creates CursesInterface instance, manages application lifecycle
nci.py → mcp.py: Sends user messages via MCPClient.send_message()
nci.py → emm.py: Stores/retrieves messages via EnhancedMemoryManager methods
nci.py → sme.py: Updates narrative pressure via StoryMomentumEngine.process_user_input()
mcp.py ← sme.py: Receives story context for enhanced prompting via get_story_context()
mcp.py ← emm.py: Receives conversation history for context via get_conversation_for_mcp()

DATA FLOW:
1. User input captured in nci.py
2. Message stored in emm.py conversation history with automatic semantic analysis
3. Narrative pressure updated in sme.py based on input analysis
4. Context gathered from emm.py and sme.py for mcp.py request
5. MCP request sent to server, response received
6. Response displayed in nci.py interface

CONFIGURATION SYSTEM:
- aurora_config.json: Application settings and MCP server configuration
- chat_history_*.json: Conversation history files with timestamp naming and metadata
- debug_*.log: Debug logging when --debug flag enabled

APPLICATION ENTRY POINT (main.py):
================================================================

MAIN CLASS STRUCTURE:
- DevNameRPGClient: Primary application coordinator
- DebugLogger: Centralized logging system shared across modules
- ApplicationConfig: JSON-based configuration management with defaults
- Command-line argument parsing with --debug and --config options

INITIALIZATION SEQUENCE:
1. Argument parsing and environment setup
2. Module verification and dependency checking
3. Configuration loading with fallback to defaults
4. Debug logger initialization if enabled
5. CursesInterface creation with config injection
6. Signal handlers for graceful shutdown

CONFIGURATION DEFAULTS:
- MCP server: http://localhost:11434/api/chat
- MCP model: qwen2.5:14b-instruct
- MCP timeout: 30 seconds
- Memory max tokens: 16000
- Interface color theme: classic
- Story pressure decay: 0.05
- Antagonist threshold: 0.6

DEPENDENCY MANAGEMENT:
- Required: curses module (with platform-specific guidance)
- Optional: httpx (graceful degradation if missing)
- Module verification system for all components

NCURSES INTERFACE MODULE (nci.py):
================================================================

INTERFACE ARCHITECTURE:
- CursesInterface: Main interface controller
- ColorManager: Theme-based color management (classic/dark/bright)
- InputValidator: Token estimation and input validation
- DisplayMessage: Message formatting for display

WINDOW MANAGEMENT:
- Output window: Conversation display with scroll support
- Input window: User input with real-time validation
- Status window: System statistics and connection status
- Automatic dimension calculation with minimum size validation

INPUT PROCESSING:
- Character-by-character input building
- Enter key submission with validation
- Arrow key scrolling through conversation history
- Command processing for slash commands (/help, /quit, /clear, /stats, /theme)

MODULE INTERCONNECTS:
- Creates and manages MCPClient, EnhancedMemoryManager, StoryMomentumEngine instances
- Coordinates message flow between all modules
- Handles async MCP requests while blocking interface input
- Formats story context from SME for MCP integration

COLOR THEME SYSTEM:
- Classic: Cyan user, green assistant, yellow system, red error, blue borders
- Dark: White user, cyan assistant, magenta system, red error, white borders
- Bright: Blue user, green assistant, yellow system, red error, magenta borders

MCP COMMUNICATION MODULE (mcp.py):
================================================================

STREAMLINED ARCHITECTURE:
- MCPClient class with essential HTTP communication only
- Single retry mechanism (MCP_MAX_RETRIES = 2) 
- Eliminated performance monitoring and verbose error reporting
- Clean context integration from both EMM and SME modules
- Brand-neutral naming throughout (DevName RPG Client)

CORE FUNCTIONALITY:
- MCPClient.send_message(): Primary interface for message transmission
  * Integrates story_context from SME module
  * Accepts conversation_history from EMM module
  * Handles up to 10 historical messages for efficiency
  * Returns clean response content or raises exception
- MCPClient.test_connection(): Async connection verification
- MCPClient.get_server_info(): Basic diagnostics without performance metrics
- MCPClient.update_system_prompt(): Dynamic prompt modification capability

ERROR HANDLING:
- Single exception type for all MCP failures
- Debug-only verbose logging (no user-facing error verbosity)
- Graceful fallback when httpx unavailable
- Brief retry delays (1 second) without complex backoff algorithms

CONTEXT INTEGRATION DESIGN:
- Story context from SME injected as system message: "Story Context: {context}"
- Conversation history from EMM limited to last 10 messages for token efficiency
- System prompt + story context + history + user input = complete message chain
- Minimal response validation for essential structure checking

ASYNC EXECUTION PATTERN:
- Creates new event loop for each request (avoiding loop conflicts)
- Simple async/await pattern with httpx.AsyncClient
- Timeout management through httpx configuration
- Clean loop closure after execution

DEFAULT SYSTEM PROMPT:
High-fantasy RPG Game Master instructions for immersive storytelling, rich descriptions, engaging NPCs, and dynamic narrative responses.

ENHANCED MEMORY MANAGER MODULE (emm.py):
================================================================

SEMANTIC ANALYSIS SYSTEM:
- LLM-powered message categorization with 6 semantic categories
- Context-aware analysis using 11-message window (5 before + target + 5 after)
- 3-tier retry system: Full Analysis → Simple Analysis → Binary Decision
- Pure LLM-driven semantic decisions with no programmatic fallbacks

SEMANTIC CATEGORIES & PRESERVATION RATIOS:
- story_critical: 0.9 (90% preservation) - Major plot developments, character deaths, world-changing events
- character_focused: 0.8 (80% preservation) - Relationship changes, character development, personality reveals
- relationship_dynamics: 0.8 (80% preservation) - Evolving relationships between characters
- emotional_significance: 0.75 (75% preservation) - Dramatic moments, trust/betrayal, conflict resolution
- world_building: 0.7 (70% preservation) - New locations, lore, cultural info, political changes
- standard: 0.4 (40% preservation) - General interactions, travel, routine activities

PROGRESSIVE CONDENSATION SYSTEM:
- Multi-pass condensation with increasing aggressiveness (up to 3 passes)
- Aggressiveness reduces preservation ratios by 0.1 per pass
- Minimum preservation ratio of 0.2 to prevent critical information loss
- Recent messages (last 5) always protected from condensation

FRAGMENTATION LOGIC:
- LLM identifies multi-category messages and fragments them appropriately
- Each fragment assigned individual categories and importance scores
- Multi-category fragments use highest applicable preservation ratio
- Example: "Maria thanked you. Leon sneers from the side" → 2 fragments with different categories

MEMORY MANAGEMENT ALGORITHM:
1. Add message → check token threshold → trigger condensation if needed
2. Analyze condensable messages (excluding recent 5) for semantic importance
3. Apply category-weighted preservation ratios adjusted for aggressiveness level
4. Group messages for condensation and create LLM-generated summary
5. Replace condensed messages with summary, preserve important messages
6. Repeat up to 3 passes if memory still exceeds threshold

THREAD SAFETY & ASYNC OPERATIONS:
- Thread-safe message storage with locking mechanisms
- Async LLM calls for semantic analysis without blocking interface
- Non-blocking condensation operations in separate event loop
- Safe concurrent access to conversation history

EMM INTERFACE FOR OTHER MODULES:
- get_conversation_for_mcp(): Returns formatted conversation for MCP requests
- add_message(): Thread-safe message storage with automatic condensation
- get_messages(): Retrieves conversation history with optional limits
- get_memory_stats(): Returns current memory utilization statistics
- save_conversation(): Persistent storage with metadata
- load_conversation(): Restore from saved files
- analyze_conversation_patterns(): Debug and analysis functionality

STORY MOMENTUM ENGINE MODULE (sme.py):
================================================================

NARRATIVE PRESSURE SYSTEM:
- Dynamic pressure tracking on 0.0-1.0 scale with pattern-based calculation
- Real-time story arc progression: Setup → Rising Action → Climax → Resolution
- Context-adaptive antagonist generation based on user input patterns
- LLM-first approach with single fallback system

PRESSURE CALCULATION ALGORITHM:
- Pattern-based analysis using 6 momentum categories: conflict, tension, mystery, exploration, social, resolution
- Weighted keyword matching with dynamic pressure deltas per category type
- Length/complexity factors and punctuation intensity analysis
- Natural pressure decay over time (0.05 per minute) to prevent artificial inflation
- Rate limiting (2-second cooldown) to prevent spam-induced pressure spikes

MOMENTUM PATTERN CATEGORIES:
- conflict: Keywords trigger +0.15 pressure (fight, attack, defend, battle, combat, strike)
- tension: Keywords trigger +0.12 pressure (danger, threat, fear, worry, concern, risk)
- mystery: Keywords trigger +0.08 pressure (strange, unusual, mysterious, hidden, secret, whisper)
- exploration: Keywords trigger +0.05 pressure (examine, search, look, investigate, explore, discover)
- social: Keywords trigger +0.03 pressure (talk, speak, negotiate, persuade, convince, ask)
- resolution: Keywords trigger -0.10 pressure (resolve, solution, answer, complete, finish, end)

STORY ARC THRESHOLDS:
- Setup Arc: 0.0-0.3 pressure (calm exploration, building tension)
- Rising Action: 0.3-0.7 pressure (escalating conflict)
- Climax: 0.7-0.9 pressure (peak intensity)
- Resolution: 0.9-1.0 pressure (concluding action)

ANTAGONIST MANAGEMENT:
- Single context-adaptive antagonist generation
- Introduction threshold: 0.6 pressure level
- Dynamic antagonist types based on recent user input analysis:
  * magical_opposition: Corrupted Mage (magic/spell keywords detected)
  * environmental_threat: Ancient Guardian (exploration/dungeon keywords)
  * social_conflict: Corrupt Official (town/people keywords)
  * adaptive_threat: Shadow Entity (default context-adaptive)
- Threat level scaling with current pressure level
- Automatic deactivation during Resolution arc

CONTEXT GENERATION FOR MCP:
- Comprehensive story context including pressure level, arc, narrative state
- Antagonist information when present (name, motivation, threat level, active status)
- Pressure trend analysis (rising/falling/stable) based on recent history
- Narrative state descriptions: calm_exploration, building_tension, escalating_conflict, peak_intensity, concluding_action
- Tension introduction flags and climax approach indicators

SME INTERFACE FOR OTHER MODULES:
- process_user_input(): Updates pressure and returns processing results
- get_story_context(): Returns story context for MCP integration
- reset_story_state(): Clears state for new sessions
- get_pressure_stats(): Returns pressure analytics and statistics
- save_state(): Persistent state management
- load_state(): State restoration
- get_debug_info(): Comprehensive debug information

PROMPT SYSTEM INTEGRATION:
================================================================

COMPANION PROMPTS:
- tsundere_companion.prompt: Seraphina knight companion template
- dandere_companion.prompt: Elena healer companion template
- kuudere_companion.prompt: Lysander mage companion template
- himedere_companion.prompt: Victoria noble companion template

RULE PROMPTS:
- critrules.prompt: Core GM rules for character control boundaries
- lowrules.prompt: World generation and environmental storytelling rules

PROMPT LOADING SYSTEM:
- companion.prompt: Active companion (copied from specific companion template)
- Dynamic prompt injection based on story context and pressure levels
- Memory-conscious prompt design for semantic preservation

MODULAR PRESERVATION RULES:
================================================================

1. Breaking interconnects between modules will cause system failure
2. Enhanced Memory Manager maintains specific interface methods for other modules
3. Story Momentum Engine maintains specific interface methods for other modules
4. MCP Communication Module maintains specific interface methods for other modules
5. LLM semantic analysis requires functional MCP configuration
6. File persistence maintains metadata for debugging and analysis

ERROR HANDLING HIERARCHY:
- main.py: Application-level errors and graceful shutdown
- nci.py: Interface errors and display failures
- mcp.py: Network errors and communication failures (simplified single exception type)
- emm.py: Memory management errors, LLM analysis failures, condensation errors
- sme.py: Narrative analysis and context generation errors

DEBUGGING INFRASTRUCTURE:
- Centralized DebugLogger class in main.py shared across all modules
- Enhanced memory debugging with semantic analysis tracking
- LLM interaction logging for prompt optimization
- Token usage monitoring and condensation effectiveness metrics
- Story pressure tracking and antagonist lifecycle monitoring
- Simplified MCP communication logging (debug-only verbosity)

PERFORMANCE CONSIDERATIONS:
- LLM semantic analysis adds latency but provides superior memory management
- Async operations prevent interface blocking during condensation
- Context window analysis balances accuracy with computational cost
- Progressive aggressiveness ensures memory constraints are met
- Story pressure calculation optimized for real-time performance
- Rate limiting prevents computational overhead from input spam
- Simplified MCP retry logic reduces communication overhead

DEPENDENCY REQUIREMENTS:
- Python 3.8+ with curses support
- httpx library for HTTP communication (required for LLM semantic analysis)
- asyncio for non-blocking LLM calls in memory management
- JSON for configuration and history storage
- Threading for thread-safe memory operations

AI MODIFICATION GUIDELINES:
================================================================

1. PRESERVE all programmatic interconnects when modifying any module
2. UPDATE this genai.txt file when making architectural changes
3. MAINTAIN the single responsibility principle for each module
4. DO NOT merge module responsibilities or break the modular boundaries
5. VERIFY that changes do not break the data flow patterns
6. TEST interconnects after any modifications to ensure system integrity
7. LLM prompt modifications require extensive testing for reliability
8. Semantic category changes must update preservation ratios accordingly
9. Story pressure thresholds and antagonist generation logic must remain context-adaptive
10. Pattern-based pressure calculation should maintain balance between categories