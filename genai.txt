# GENAI.TXT - AI ANALYSIS REFERENCE FOR DEVNAME RPG CLIENT
================================================================
*Last Updated: December 2024 - Based on Current Codebase Analysis*

## PROJECT OVERVIEW
Terminal-based RPG storytelling client using Large Language Models through MCP (Model Context Protocol). Features hub-and-spoke orchestration, pattern-based semantic analysis, story momentum tracking, and serial LLM request processing with immediate user echo.

## CRITICAL ARCHITECTURE: HUB-AND-SPOKE PATTERN

**Enforced Rules:**
- ALL inter-module communication flows through `orch.py`
- NO direct module-to-module calls allowed
- MCP access is orchestrator-exclusive via serial queue
- UI maintains zero message state (fetches on-demand)

## PROGRAM EXECUTION FLOW

### Startup Sequence
```
main.py → Orchestrator.__init__() → Module initialization → UI.run() → Event loop
```

1. **Module Verification**: Check all `.py` files present
2. **Prompt Loading**: Concatenate prompts with `/no_think` prefix
3. **Configuration**: Hardcoded MCP at localhost:3456, model: qwen3:8b-q4_K_M
4. **Module Init Order**: EMM → SEM → SME → MCP → NCUI
5. **Background Services**: LLM worker, analysis worker, auto-save
6. **UI Launch**: Transfer control to NCursesUIController

## MODULE ARCHITECTURE

### Hub Module: orch.py (Orchestrator)

**Core Responsibilities:**
- Serial LLM request queue management (PriorityQueue)
- Immediate pattern-based semantic analysis
- Background thread coordination
- Module callback routing
- State synchronization

**Critical Methods:**
```python
_handle_ui_callback()      # Route UI requests
_process_user_input()      # Store user msg → queue LLM
_llm_worker()             # Serial LLM processing thread
_perform_immediate_semantic_analysis()  # Pattern matching
_make_llm_request()       # ONLY function calling MCP
_check_resolution_guidance()  # SME integration
```

**LLM Queue System:**
- Priority levels: 1=user response, 3=semantic, 5=condensation
- Serial processing ensures no concurrent LLM calls
- Request tracking with IDs and timestamps
- Worker thread auto-restarts if dies

### Spoke Modules

#### ncui.py - Stateless UI Controller
**Implementation:**
- Fetches messages from orchestrator every cycle
- No local message storage (stateless)
- Immediate user echo via orchestrator storage
- PgUp/PgDn scrollback navigation
- Multi-line input with viewport scrolling

**Known Issues:**
- Home/End keys detected but not handled (codes 262/360)
- Ctrl+arrow word jumping not working (codes 554/569)

#### emm.py - Enhanced Memory Manager
**Current State:**
- Thread-safe storage with RLock
- 25,000 token budget (never reached in practice)
- Message fields: `importance_score`, `semantic_metadata`, `content_category`
- Auto-save with atomic file operations
- Condensation methods exist but unused

**Message Structure:**
```python
Message:
  id: UUID
  content: str
  message_type: MessageType enum
  timestamp: ISO format
  token_estimate: chars/4
  content_category: str (semantic category)
  importance_score: float (0.0-1.0)
  semantic_metadata: dict
  condensed: bool
```

#### sem.py - Semantic Analysis Engine
**Current Implementation:**
- `validate_input()`: Categorizes user input type
- Pattern-based categorization (no LLM usage)
- Six categories with preservation ratios
- Called synchronously by orchestrator after each message

**Semantic Categories:**
```
story_critical:        0.9 preservation, 0.9 importance
character_focused:     0.8 preservation, 0.8 importance
relationship_dynamics: 0.7 preservation, 0.7 importance
emotional_significance: 0.6 preservation, 0.6 importance
world_building:        0.5 preservation, 0.5 importance
standard:             0.4 preservation, 0.4 importance
```

**Unused Features:**
- LLM-based analysis defined but never called
- Orchestrator callback system not integrated
- Condensation preservation ratios not applied

#### sme.py - Story Momentum Engine
**Working Features:**
- Pressure tracking (0.0-1.0) with pattern detection
- Narrative time accumulation
- Story arc progression
- Resolution guidance at pressure >= 1.0
- Basic antagonist tracking

**Pattern Categories:**
- Tension patterns: +0.06 to +0.15 pressure
- Conflict patterns: +0.08 to +0.20 pressure
- Resolution patterns: -0.05 to -0.15 pressure
- Natural decay: -0.02 per exchange

**Integration Gaps:**
- Momentum state minimally affects LLM context
- Resolution guidance not properly injected
- `analyze_momentum()` rarely called

#### mcp.py - MCP Client
**Configuration:**
- Server: http://localhost:3456/chat
- Model: qwen3:8b-q4_K_M
- Timeout: 300 seconds
- System prompts concatenated from all prompt files
- Think block removal from responses

## DATA FLOW PIPELINES

### User Input Pipeline (ACTUAL)
```
User Input → ncui.py → orch._handle_ui_callback()
                ↓
            emm.add_message(USER) [IMMEDIATE]
                ↓
            _perform_immediate_semantic_analysis() [SYNC]
                ↓
            Queue LLM request (priority=1)
                ↓
            Return success to UI [USER MSG AVAILABLE]
                ↓
            LLM worker processes queue
                ↓
            mcp.send_message() [BLOCKING]
                ↓
            emm.add_message(ASSISTANT)
                ↓
            _perform_immediate_semantic_analysis() [SYNC]
                ↓
            UI fetches updated messages
```

### Analysis Pipeline (PARTIALLY IMPLEMENTED)
```
Every 15 messages → _trigger_periodic_analysis()
                ↓
            sem.analyze_conversation() [Pattern-based only]
                ↓
            sme.analyze_momentum() [Pattern detection]
                ↓
            Results stored but not used
```

## CURRENT IMPLEMENTATION STATUS

### ✅ Working
- User input echo (immediate storage before LLM)
- Serial LLM processing (no concurrency issues)
- Pattern-based semantic categorization
- Basic momentum tracking
- File persistence with auto-save
- Stateless UI with orchestrator fetch

### ⚠️ Partially Working
- Semantic categories assigned but not used
- Momentum calculated but minimal LLM impact
- Resolution guidance triggers but not injected properly
- Periodic analysis runs but results ignored

### ❌ Not Working
- Memory condensation (threshold never reached)
- LLM-based semantic analysis (never called)
- Preservation ratios (not applied)
- Actual summarization (placeholder only)
- Home/End key navigation in input
- Ctrl+arrow word jumping

## TOKEN BUDGET MANAGEMENT

### Current State
- **Context Window**: 32,000 tokens
- **System Prompts**: ~5,000 tokens (3 files concatenated)
- **Memory Budget**: 25,000 tokens (never reached)
- **Typical Usage**: <1,000 tokens in practice
- **Condensation**: Never triggers

### Why Condensation Doesn't Work
1. Threshold too high (25,000 tokens)
2. No actual summarization implementation
3. Preservation ratios defined but not applied
4. Semantic categories not used for selection

## CRITICAL FIXES NEEDED

### Priority 1: Fix Condensation
```python
# In emm.py
self.max_memory_tokens = 2000  # Lower for testing
# Implement actual summarization
# Apply preservation ratios
```

### Priority 2: Connect Momentum to LLM
```python
# In orch.py _make_llm_request()
# Properly format momentum state for system prompt
# Inject resolution guidance when triggered
```

### Priority 3: Use Semantic Analysis
```python
# In orch.py
# Queue semantic enhancement for low-confidence patterns
# Apply categories to condensation selection
```

## TESTING POINTS

### Semantic System
- Check `memory.json` for importance_score values
- Verify categories match content patterns
- Test with `/semantic` command
- Lower token limit to force condensation

### Momentum System
- Monitor pressure in debug.log
- Check for resolution guidance at pressure=1.0
- Verify story arc transitions
- Test antagonist activation at pressure>0.5

### Queue System
- Monitor queue size in stats
- Check worker thread status
- Verify serial processing (no concurrent LLM calls)
- Test request priority ordering

## FILE STRUCTURE
```
Required:
├── main.py             # Entry point
├── orch.py             # Orchestrator hub
├── ncui.py             # Stateless UI
├── uilib.py            # UI components
├── emm.py              # Memory manager
├── sem.py              # Semantic analysis
├── sme.py              # Momentum engine
├── mcp.py              # LLM client
└── critrules.prompt    # Core GM rules

Optional:
├── companion.prompt    # NPC definitions
├── lowrules.prompt     # Narrative style
├── memory.json         # Persistence
└── debug.log           # Debug output
```

## DEVELOPMENT NOTES

### Architecture Principles
1. **Hub-and-spoke is absolute** - No direct module communication
2. **UI is stateless** - Always fetch from orchestrator
3. **LLM calls are serial** - One request at a time
4. **Semantic analysis is synchronous** - Pattern-based, immediate
5. **Background threads are supervised** - Auto-restart if needed

### Current Bottlenecks
- Serial LLM processing (intentional, prevents races)
- Pattern-based semantic analysis (LLM version unused)
- Condensation never triggers (threshold issue)
- Momentum state underutilized (minimal LLM impact)

### Next Steps
1. Implement test mode with low token limits
2. Create actual summarization logic
3. Connect momentum state to LLM prompts properly
4. Fix input navigation keys
5. Add semantic enhancement queue for uncertain patterns

---
*Architecture: Hub-and-spoke with serial LLM processing*
*Semantic: Pattern-based categorization (LLM unused)*
*Momentum: Tracked but underutilized*
*Status: Functional but condensation/integration incomplete*
```

## Summary of Key Findings

1. **Semantic Memory Condensation** is architecturally present but functionally broken:
   - Categories are assigned via patterns but never used for preservation
   - Token limit (25,000) is too high to ever trigger in practice
   - No actual summarization logic exists

2. **Story Momentum Engine** calculates pressure but has minimal impact:
   - Pressure tracked accurately with patterns
   - Resolution guidance triggers but isn't properly injected into prompts
   - Story context passed to MCP but in minimal format

3. **Integration gaps** prevent these systems from working together:
   - Semantic categories don't influence condensation selection
   - Momentum state doesn't meaningfully affect LLM behavior
   - Background analysis results are calculated but ignored

The system works for basic conversation but the advanced memory and narrative features are only partially implemented.
