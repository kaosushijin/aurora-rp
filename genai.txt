# GENAI.TXT - AI CONTEXT REFERENCE FOR DEVNAME RPG CLIENT
================================================================================
**CRITICAL NOTICE**: This document serves as the primary reference for generative AI to understand the complete project scope beyond its active context window. It enforces module boundaries and prevents regressions when modifying functions that affect multiple modules. Always consult this document before making changes that could ripple across the codebase.

## PROJECT OVERVIEW
Terminal-based RPG storytelling client using Large Language Models via MCP (Model Context Protocol). 
Architecture: Hub-and-spoke pattern with centralized orchestration through `orch.py`.
Status: Remodularization complete, debug logger interface standardization required.

## COMPLETE MODULE REGISTRY AND INTERCONNECTS

### FILE STRUCTURE (All in root directory)
```
aurora-rp/
├── main.py              # Entry point, prompt management, debug logger
├── orch.py              # Central hub orchestrator - ONLY module calling mcp.py
├── mcp.py               # LLM communication - accessed ONLY by orch.py
├── ncui.py              # Pure UI controller (imports uilib.py)
├── uilib.py             # Consolidated UI components library
├── emm.py               # Enhanced memory manager
├── sme.py               # Story momentum engine
├── sem.py               # Semantic analysis engine
├── critrules.prompt     # Core rules (REQUIRED)
├── lowrules.prompt      # Additional rules (optional)
├── companion.prompt     # Companion definitions (optional)
├── genai.txt            # This reference document
└── legacyref/           # Legacy code reference (do not modify)
```

### MODULE INTERCONNECTION MATRIX

#### main.py
**Purpose**: Application entry, prompt loading, debug logging
**Imports**: `orch.Orchestrator`, `mcp.MCPClient`
**Exports**: `DebugLogger`, `PromptManager`, `DevNameRPGClient`
**Data Sent**: 
- To orch.py: `config` dict, `loaded_prompts` dict, `debug_logger` object
**Data Received**: 
- From orch.py: Integer exit code
**Critical Functions**:
- `load_and_optimize_prompts()`: Condenses prompts to 5000 token budget
- `validate_prompt_files()`: Ensures critrules.prompt exists
**Dependencies**: Python 3.8+, asyncio, pathlib

#### orch.py (CENTRAL HUB)
**Purpose**: Central orchestration of all service modules
**Imports**: `ncui`, `emm`, `sme`, `sem`, `mcp`
**Exports**: `Orchestrator`, `OrchestrationState`
**Data Sent**:
- To ncui.py: Display messages, UI commands
- To emm.py: Messages for storage, memory operations
- To sme.py: Story state updates, analysis triggers
- To sem.py: Analysis requests
- To mcp.py: LLM request payloads (EXCLUSIVE ACCESS)
**Data Received**:
- From ncui.py: User input strings, UI events
- From emm.py: Message history, memory state
- From sme.py: Momentum analysis results
- From sem.py: Semantic categorization results
- From mcp.py: LLM responses
**Critical Functions**:
- `initialize_modules()`: Sets up all service modules with callbacks
- `process_user_input()`: Routes input through processing pipeline
- `trigger_periodic_analysis()`: Manages 15-message analysis cycles
- `make_llm_request()`: ONLY function that calls mcp.py
**Threading**: Background analysis thread for non-blocking operations

#### mcp.py
**Purpose**: HTTP communication with Node.js ollama MCP server
**Imports**: httpx (optional), asyncio
**Exports**: `MCPClient`, `MCPState`
**Data Sent**:
- To ollama server: HTTP POST requests with prompts
**Data Received**:
- From orch.py: System prompts, user messages, context
- From ollama server: JSON responses with generated text
**Critical Functions**:
- `send_message()`: Async HTTP request with retry logic
- `parse_response()`: 5-strategy JSON parsing for reliability
- `build_context()`: Assembles prompts with token management
**Configuration**:
- Server URL: http://127.0.0.1:3456/chat
- Model: qwen2.5:14b-instruct-q4_k_m
- Timeout: 300 seconds
- Context window: 32,000 tokens

#### ncui.py
**Purpose**: Terminal UI controller using ncurses
**Imports**: `uilib` components, curses
**Exports**: `NCursesUIController`
**Data Sent**:
- To orch.py: User input, UI commands
**Data Received**:
- From orch.py: Messages to display, UI updates
- From uilib.py: UI component instances
**Critical Functions**:
- `initialize_ui()`: Sets up curses windows
- `process_input()`: Handles keyboard input
- `display_message()`: Renders messages to screen
- `handle_resize()`: Manages terminal resize events

#### uilib.py
**Purpose**: Consolidated UI component library
**Imports**: curses, standard library only
**Exports**: `TerminalManager`, `ColorManager`, `ScrollManager`, `MultiLineInput`, `DisplayMessage`
**Data Sent**:
- To ncui.py: Component instances and methods
**Data Received**:
- Configuration parameters from ncui.py
**Critical Components**:
- `TerminalManager`: Dynamic coordinate calculation
- `ColorManager`: Theme management (classic/dark/bright)
- `ScrollManager`: Scroll position with bounds checking
- `MultiLineInput`: Multi-line input with word wrap
- `DisplayMessage`: Message formatting and wrapping

#### emm.py
**Purpose**: Message storage and memory management
**Imports**: Standard library only
**Exports**: `EnhancedMemoryManager`, `Message`, `MessageType`
**Data Sent**:
- To orch.py: Message history, memory state
**Data Received**:
- From orch.py: New messages, memory operations
**Critical Functions**:
- `add_message()`: Store with metadata and token estimation
- `get_conversation_context()`: Retrieve for LLM context
- `save_conversation()`: Persist to JSON file
- `condense_memory()`: Reduce when approaching limits
**Threading**: Background auto-save every 30 seconds
**File Operations**: memory.json for persistence

#### sme.py
**Purpose**: Story momentum and narrative tracking
**Imports**: Standard library only
**Exports**: `StoryMomentumEngine`, `Antagonist`, `StoryArc`
**Data Sent**:
- To orch.py: Momentum analysis results
**Data Received**:
- From orch.py: Story events, analysis triggers
**Critical Functions**:
- `analyze_momentum()`: Evaluate narrative progression
- `generate_antagonist()`: Create story antagonists
- `track_narrative_time()`: Monitor story timeline
- `calculate_pressure()`: Determine story tension
**Analysis Cycle**: Every 15 messages

#### sem.py
**Purpose**: Semantic analysis and categorization
**Imports**: Standard library only
**Exports**: `SemanticAnalysisEngine`, `SemanticAnalysisRequest`, `SemanticAnalysisResult`
**Data Sent**:
- To orch.py: Analysis results, categorizations
**Data Received**:
- From orch.py: Content for analysis
**Critical Functions**:
- `categorize_content()`: Assign semantic categories
- `analyze_importance()`: Determine preservation priority
- `condense_content()`: Intelligently reduce text
**Categories**: story_critical, character_focused, relationship_dynamics, emotional_significance, world_building, standard

## CRITICAL INTERFACE CONTRACTS

### Debug Logger Interface (NEEDS STANDARDIZATION)
**Current Issue**: Mixed calling patterns causing startup failure

**Required Pattern** (Method-based):
```python
self.debug_logger.debug(message, category)
self.debug_logger.error(message, category)
self.debug_logger.system(message)
```

**Files Requiring Fix**:
- orch.py: Line ~385 - Change `self.debug_logger(f"[ORCHESTRATOR] {message}")` to `self.debug_logger.debug(message, "ORCHESTRATOR")`
- ncui.py: All debug calls - Change callable pattern to method pattern

### Service Module Initialization Pattern
All service modules must implement:
```python
def __init__(self, debug_logger=None):
    self.debug_logger = debug_logger
    self.orchestrator_callback = None

def set_orchestrator_callback(self, callback):
    self.orchestrator_callback = callback
```

### LLM Request Pattern (EXCLUSIVE)
**ONLY orch.py may call mcp.py functions**
All other modules must use:
```python
result = self.orchestrator_callback('llm_request', {
    'prompt': prompt_text,
    'context': context_data
})
```

## DATA FLOW SEQUENCES

### User Input Processing
1. ncui.py captures keyboard input
2. ncui.py sends to orch.py via callback
3. orch.py validates through sem.py
4. orch.py stores in emm.py
5. orch.py sends to mcp.py for LLM processing
6. mcp.py returns response to orch.py
7. orch.py stores response in emm.py
8. orch.py sends to ncui.py for display

### Periodic Analysis (15-message cycle)
1. orch.py monitors message count
2. At 15 messages, triggers background thread
3. Thread collects context from emm.py
4. Thread requests analysis from sme.py
5. sme.py returns momentum state
6. orch.py updates emm.py with state
7. orch.py may trigger antagonist generation

### Memory Condensation
1. emm.py detects token limit approach
2. emm.py requests categorization from sem.py via orch.py
3. sem.py analyzes and categorizes messages
4. emm.py condenses based on categories
5. emm.py saves condensed state to disk

## CONFIGURATION PARAMETERS

### Token Budgets
- Context Window: 32,000 tokens
- System Prompt: 5,000 tokens max
- User Input: 2,000 tokens max
- Memory Threshold: 20,000 tokens

### Timing Parameters
- Auto-save: Every 30 seconds
- Momentum Analysis: Every 15 messages
- Connection Timeout: 300 seconds
- Retry Delay: 2 seconds

### File Paths
- Memory: memory.json
- Debug Log: debug.log
- Prompts: *.prompt (same directory)

## KNOWN ISSUES AND FIXES

### Issue 1: Debug Logger Interface Mismatch
**Status**: CRITICAL - Blocks startup
**Fix Required**: Standardize all modules to use method-based pattern
**Files**: orch.py (line ~385), ncui.py (all debug calls)
**Time to Fix**: 5 minutes

### Issue 2: Prompt Path References
**Status**: RESOLVED in current version
**Note**: All prompt files must be in root directory

### Issue 3: Import Path Issues
**Status**: RESOLVED in current version
**Note**: All modules import from same directory

## REGRESSION PREVENTION RULES

### Before Modifying Any Function
1. Check this document for module interconnects
2. Identify all modules that call the function
3. Identify all modules the function calls
4. Consider data structure changes
5. Verify threading implications
6. Test with debug mode enabled

### Hub-and-Spoke Enforcement
- NEVER allow direct module-to-module calls except through orch.py
- NEVER allow any module except orch.py to import mcp.py
- NEVER bypass the orchestrator callback pattern
- ALWAYS route cross-module communication through orchestrator

### Threading Safety
- emm.py uses threading.Lock for all operations
- Background threads must not modify UI directly
- Analysis threads communicate results via callbacks
- Auto-save runs independently with atomic writes

### Error Handling Patterns
- All modules must handle None debug_logger
- Network failures must not crash the application
- File I/O errors must be logged but not fatal
- UI resize must not lose message history

## TESTING CHECKLIST

### Startup Sequence
- [ ] All modules import successfully
- [ ] Debug logger initializes without errors
- [ ] Prompts load from correct directory
- [ ] Orchestrator initializes all modules
- [ ] UI displays without crashes

### Core Functionality
- [ ] User input processes correctly
- [ ] LLM responses display properly
- [ ] Messages save to memory.json
- [ ] 15-message analysis triggers
- [ ] Theme switching works

### Error Scenarios
- [ ] Missing prompt files handled gracefully
- [ ] Network timeout doesn't crash
- [ ] Terminal resize maintains state
- [ ] Ctrl+C shuts down cleanly

## VERSION HISTORY

### Current Version (Commit: 5170347a6ddd490afb877493032367bf6602828c)
- Architecture: Hub-and-spoke pattern complete
- Status: Debug logger interface fix required
- Modules: 8 primary Python files
- Dependencies: Python 3.8+, curses, httpx (optional)

### Previous Working Version (legacyref/)
- Architecture: Direct module interconnections
- Status: Fully functional but less maintainable
- Preserved for reference and rollback

---
**END OF GENAI.TXT** - Last Updated: Analysis of repository state
**Remember**: This document is the authoritative source for understanding module interactions and preventing regressions. Always consult before making changes that could affect multiple modules.