# GENAI.TXT - AI ANALYSIS REFERENCE FOR DEVNAME RPG CLIENT
================================================================

## PROJECT SCOPE
Terminal-based RPG storytelling client leveraging Large Language Model capabilities through MCP (Model Context Protocol). Features hub-and-spoke orchestration architecture with stateless UI controller, comprehensive semantic analysis, and responsive multi-threaded processing.

## PROGRAM FLOW ANALYSIS (Commit: e653bab0ef89fee9fb1c810c471628c4c32849b0)

### Application Entry Point (main.py)
1. **Module Verification**: Validate all remodularized components present
2. **Prompt Management**: Load prompt files with automatic token optimization
3. **Configuration**: Hardcoded values (MCP server: localhost:3456, model: qwen2.5:14b-instruct-q4_k_m)
4. **Orchestrator Initialization**: Create hub controller with all service modules
5. **UI Launch**: Transfer control to stateless NCursesUIController

### Core Execution Flow
```
main() → DevNameRPGClient.run() → Orchestrator.run() → NCursesUIController.run() → UI event loop
```

## REMODULARIZED ARCHITECTURE - STATELESS UI WITH HUB-AND-SPOKE PATTERN

### Hub Module (Central Coordinator)

**orch.py** - Central orchestrator hub for all module coordination
- Exclusive access to MCP client for LLM communication
- Manages complete input → semantic → storage → LLM → display pipeline
- Coordinates background analysis threads (15-message momentum cycles)
- Handles all cross-module communication through callbacks
- **Stateless UI Support**: Provides message retrieval and state queries for UI
- **Key Methods**:
  - `_handle_ui_callback()`: Processes all UI requests
  - `get_messages()`: Returns display-ready messages for stateless UI
  - `get_display_status()`: Provides current system state
  - `_trigger_background_analysis()`: Manages async LLM operations

### Primary Spoke Modules

**main.py** - Application lifecycle and environment management
- `PromptManager`: Token-aware prompt loading with condensation
- `DevNameRPGClient`: Application coordinator with signal handling
- `ApplicationConfig`: Hardcoded configuration management
- **Token Budget**: 5,000 tokens for system prompts with auto-optimization

**ncui.py** - STATELESS NCurses UI Controller
- **No message storage** - retrieves all content from orchestrator on-demand
- Terminal management, window creation, pure display logic
- Multi-line input with intelligent submission detection
- Color themes (classic/dark/bright) with dynamic switching
- Scrolling with PgUp/PgDn, Home/End navigation
- Command system (/help, /stats, /theme, /analyze, /quit)
- **Stateless Operations**:
  - `_process_display_updates()`: Fetches fresh messages each cycle
  - `_handle_user_input()`: Passes input directly to orchestrator
  - No display_buffer or message_ids tracking

**uilib.py** - Consolidated UI component library
- `calculate_box_layout()`: Dynamic terminal geometry calculations
- `TerminalManager`: Size validation and resize handling
- `ColorManager`: Three built-in color schemes with hot-swapping
- `ScrollManager`: Protected scroll state with bounds checking
- `MultiLineInput`: Advanced text input with word wrapping
- `DisplayMessage`: Type-aware message formatting
- **Constants**: MIN_SCREEN_WIDTH=80, MIN_SCREEN_HEIGHT=24

### Backend Service Modules

**emm.py** - Enhanced Memory Manager (storage spoke)
- Thread-safe message storage with RLock protection
- Background auto-save to memory.json (non-blocking)
- 25,000 token memory budget with condensation triggers
- Message types: USER, ASSISTANT, SYSTEM, MOMENTUM_STATE
- **Orchestrator Integration**: Semantic categorization via callbacks
- **State Persistence**: SME momentum state storage/retrieval

**sme.py** - Story Momentum Engine (narrative spoke)
- Pattern-based momentum analysis with LLM enhancement
- Story arc tracking: SETUP→RISING→CLIMAX→RESOLUTION
- Pressure system (0.0-1.0) with floor ratcheting
- Narrative time tracking with duration patterns
- Antagonist management with threat assessment
- **Simplified**: Basic threshold detection replaces complex LLM generation

**sem.py** - Semantic Analysis Engine (analysis spoke)
- Input validation with token counting
- 6-tier semantic categorization hierarchy:
  - story_critical (90% preservation)
  - character_focused (80% preservation)
  - relationship_dynamics (70% preservation)
  - emotional_significance (60% preservation)
  - world_building (50% preservation)
  - standard (40% preservation)
- 5-strategy JSON parsing for LLM reliability
- Content condensation prompt generation

**mcp.py** - MCP Client (LLM communication spoke)
- HTTP-based Model Context Protocol client
- Exclusive orchestrator access (no direct spoke usage)
- 32,000 token context window management
- 5-minute timeout for ollama processing
- Graceful fallback without httpx dependency

## CRITICAL ARCHITECTURAL RULES

### Hub-and-Spoke Communication
1. **ALL modules communicate through orch.py** - no direct module-to-module
2. **MCP access is orchestrator-exclusive** - spokes never call LLM directly
3. **UI is purely stateless** - fetches everything from orchestrator
4. **Semantic operations are coordinated** - sem.py works via orchestrator

### Data Flow Pipeline
```
User Input → ncui.py → orch.py → sem.py → emm.py → mcp.py → emm.py → ncui.py
                         ↓
                      sme.py (parallel analysis)
```

### Module Initialization Order (in orch.py)
1. EnhancedMemoryManager (independent storage)
2. SemanticAnalysisEngine (independent analysis)
3. StoryMomentumEngine (requires Lock for thread safety)
4. MCPClient (configured with prompts)
5. NCursesUIController (requires orchestrator callback)
6. Background services startup

## TECHNICAL SPECIFICATIONS

### Core Requirements
- **Python 3.8+** with curses support
- **httpx** (optional but recommended for MCP)
- **asyncio** for async operations
- **threading** for background processing
- **json** for persistence

### Token Management
- **Context Window**: 32,000 tokens (theoretical)
- **Safe Budget**: 25,000 tokens (conservative)
- **System Prompts**: 5,000 tokens
- **User Input Max**: 2,000 tokens
- **LLM Response Reserve**: 5,000 tokens

### Performance Constraints
- **Minimum Terminal**: 80x24 characters
- **Analysis Interval**: Every 15 messages
- **Auto-save Frequency**: After each message (background)
- **Semantic Cooldown**: 30 seconds between analyses

### File Structure
```
Required Files (root directory):
- main.py            # Entry point
- orch.py           # Orchestrator hub
- ncui.py           # Stateless UI
- uilib.py          # UI components
- emm.py            # Memory manager
- sme.py            # Momentum engine
- sem.py            # Semantic engine
- mcp.py            # MCP client
- critrules.prompt  # Required system prompt

Optional Files:
- companion.prompt   # Character definitions
- lowrules.prompt   # Narrative guidelines
- memory.json       # Conversation persistence
- debug.log         # Debug output
```

## IMPLEMENTATION HIGHLIGHTS

### Stateless UI Architecture
The ncui.py module maintains **zero message state**, instead:
- Fetches messages on-demand via orchestrator callback
- Processes display updates every cycle with fresh data
- Maintains only UI state (cursor, scroll, colors)
- Eliminates memory growth from display buffers

### Thread Safety Patterns
- **EMM**: RLock for all message operations
- **SME**: Lock for state modifications
- **Orchestrator**: Thread management for background tasks
- **UI**: Single-threaded with async callbacks

### Error Recovery
- **Terminal resize**: Dynamic recalculation with validation
- **MCP timeout**: Graceful fallback with error messages
- **JSON parsing**: 5-strategy progressive fallback
- **File operations**: Atomic writes with temp files

## KNOWN ISSUES AND FUTURE IMPROVEMENTS

### Current Limitations
1. **Token Coordination**: EMM and MCP don't share token budgets
2. **Antagonist System**: Simplified threshold-based generation
3. **Time Model**: Basic pattern matching for duration
4. **Command Coverage**: Limited user commands implemented

### Planned Enhancements
1. **Scrollable Input**: Implement viewport-based input scrolling (see ncurses_input_improvements.md)
2. **Token Manager**: Centralized budget tracking system
3. **Enhanced Commands**: Story control, save/load, export
4. **Antagonist AI**: Restore full LLM-powered generation
5. **Performance Metrics**: Add timing and resource tracking

### Input System Improvements (from ncurses_input_improvements.md)
- **Viewport Scrolling**: Navigate long input with arrow keys
- **Buffer Management**: Full content access beyond window bounds
- **Enhanced Navigation**: Ctrl+arrows for word jumping
- **Smart Editing**: Backspace/Delete with word wrap adjustment
- **Boundary Handling**: Proper start/end buffer detection

## DEVELOPMENT GUIDELINES

### Architecture Preservation
- **Maintain hub-and-spoke pattern** - resist direct module communication
- **Keep UI stateless** - all data from orchestrator
- **Preserve thread safety** - use proper locking
- **Update genai.txt** - document architectural changes

### Testing Priorities
1. **Stateless UI**: Verify no message accumulation
2. **Thread Safety**: Test concurrent operations
3. **Token Limits**: Validate context window management
4. **Error Paths**: Test graceful degradation

### Code Quality Standards
- **Clear separation of concerns** between modules
- **Comprehensive error handling** with logging
- **Defensive programming** for external dependencies
- **Documentation** of architectural decisions

---
*Last Updated: Analysis of commit e653bab0ef89fee9fb1c810c471628c4c32849b0*
*Project State: Stateless UI architecture with complete hub-and-spoke orchestration*